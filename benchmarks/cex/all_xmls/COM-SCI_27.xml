<?xml version='1.0' encoding='UTF-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</title>
				<funder ref="#_PSkH5bh #_jSmPMHh #_cTZnh9a">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_UEjfQgs">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder>
					<orgName type="full">Alfred P. Sloan Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Google Research Awards</orgName>
				</funder>
				<funder>
					<orgName type="full">Hertz Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computing Machinery</publisher>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>	<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pratul</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>	<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rodrigo</forename><surname>Ortiz-Cayon</surname></persName>
							<affiliation key="aff1">
								 <orgName type="institution">Fyusion Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nima</forename><forename type="middle">Khademi</forename><surname>Kalantari</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>	<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ren</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>	<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
							<affiliation key="aff1">
								 <orgName type="institution">Fyusion Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Transactions on Graphics (TOG)</title>
						<title level="j" type="abbrev">ACM Trans. Graph.</title>
						<idno type="ISSN">0730-0301</idno>
						<imprint>
							<publisher>Association for Computing Machinery</publisher>
							<biblScope unit="volume">38</biblScope>
							<biblScope unit="issue">4</biblScope>
							<date type="published" when="2019-07"/>
						</imprint>
					</monogr>
					<idno type="MD5">FD1859A3A6DA160175D3611D08C4779F</idno>
					<idno type="DOI">10.1145/3306346.3322980</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>view synthesis</term>
					<term>plenoptic sampling</term>
					<term>light fields</term>
					<term>image-based rendering</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fast and easy handheld capture with guideline: closest object moves at most D pixels between views Promote sampled views to local light field via layered scene representation Blend neighboring local light fields to render novel views</p><p>Fig. 1. We present a simple and reliable method for view synthesis from a set of input images captured by a handheld camera in an irregular grid pattern. We theoretically and empirically demonstrate that our method enjoys a prescriptive sampling rate that requires 4000× fewer input views than Nyquist for high-fidelity view synthesis of natural scenes. Specifically, we show that this rate can be interpreted as a requirement on the pixel-space disparity of the closest object to the camera between captured views (Section 3). After capture, we expand all sampled views into layered representations that can render high-quality local light fields. We then blend together renderings from adjacent local light fields to synthesize dense paths of new views (Section 4). Our rendering consists of simple and fast computations (homography warping and alpha compositing) that can generate new views in real-time. We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound * Denotes equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The most compelling virtual experiences completely immerse the viewer in a scene, and a hallmark of such experiences is the ability to view the scene from a close interactive distance. This is currently possible with synthetically rendered scenes, but this level of intimacy has been very difficult to achieve for virtual experiences of real world scenes.</p><p>Ideally, we could simply sample the scene's light field and interpolate the relevant captured images to render new views. Such light field sampling strategies are particularly appealing because they pose the problem of image-based rendering (IBR) in a signal processing framework where we can directly reason about the density and pattern of sampled views required for any given scene. However, Nyquist rate view sampling is intractable for scenes with content at interactive distances, as the required view sampling rate increases linearly with the reciprocal of the closest scene depth. For example, for a scene with a subject at a depth of 0.5 meters captured by a mobile phone camera with a 64 • field of view and rendered at 1 megapixel resolution, the required sampling rate is an intractable 2.5 million images per square meter. Since it is not feasible to capture all the required images, the IBR community has moved towards view synthesis algorithms that leverage geometry estimation to predict the missing views.</p><p>State-of-the-art algorithms pose the view synthesis problem as the prediction of novel views from an unstructured set or arbitrarily sparse grid of input camera views. While the generality of this problem statement is appealing, abandoning a plenoptic sampling framework sacrifices the crucial ability to rigorously reason about the view sampling requirements of these methods and predict how their performance will be affected by the input view sampling pattern. When faced with a new scene, users of these methods are limited to trial-and-error to figure out whether a set of sampled views will produce acceptable results for a virtual experience.</p><p>Instead, we propose a view synthesis approach that is grounded within a plenoptic sampling framework and can precisely prescribe how densely a user must capture a given scene for reliable rendering performance. Our method is conceptually simple and consists of two main stages. We first use a deep network to promote each source view to a layered representation of the scene that can render a limited range of views, advancing recent work on the multiplane image (MPI) representation <ref type="bibr" target="#b50">[Zhou et al. 2018]</ref>. We then synthesize novel views by blending renderings from adjacent layered representations.</p><p>Our theoretical analysis shows that the number of input views required by our method decreases quadratically with the number of planes we predict for each layered scene representation, up to limits set by the camera field of view. We empirically validate our analysis and apply it in practice to render novel views with the same perceptual quality as Nyquist view sampling while using up to 64 2 ≈ 4000× fewer images.</p><p>It is impossible to break the Nyquist limit with full generality, but we show that it is possible to achieve Nyquist level performance with greatly reduced view sampling by specializing to the subset of natural scenes. This capability is primarily due to our deep learning pipeline, which is trained on renderings of natural scenes to estimate high quality layered scene representations that produce locally consistent light fields.</p><p>In summary, our key contributions are:</p><p>(1) An extension of plenoptic sampling theory that directly specifies how users should sample input images for reliable high quality view synthesis with our method.</p><p>(2) A practical and robust solution for capturing and rendering complex real world scenes for virtual exploration.</p><p>(3) A demonstration that carefully crafted deep learning pipelines using local layered scene representations achieve state-of-theart view synthesis results.</p><p>We extensively validate our derived prescriptive view sampling requirements and demonstrate that our algorithm quantitatively outperforms traditional light field reconstruction methods as well as state-of-the-art view interpolation algorithms across a range of sub-Nyquist view sampling rates. We highlight the practicality of our method by developing an augmented reality app that implements our derived sampling guidelines to help users capture input images that produce reliably high-quality renderings with our algorithm. Additionally, we develop mobile and desktop viewer apps that render novel views from our predicted layered representations in real-time. Finally, we qualitatively demonstrate that our algorithm reliably produces state-of-the-art results across a diverse set of complex real-world scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Image-based rendering (IBR) is the fundamental computer graphics problem of rendering novel views of objects and scenes from sampled views. We find that it is useful to categorize IBR algorithms by the extent to which they use explicit scene geometry, as done by <ref type="bibr" target="#b36">Shum and Kang [2000]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Plenoptic Sampling and Reconstruction</head><p>Light field rendering <ref type="bibr" target="#b26">[Levoy and Hanrahan 1996]</ref> eschews any geometric reasoning and simply samples images on a regular grid so that new views can be rendered as slices of the sampled light field. Lumigraph rendering <ref type="bibr" target="#b11">[Gortler et al. 1996]</ref> showed that using approximate scene geometry can ameliorate artifacts due to undersampled or irregularly sampled views.</p><p>The plenoptic sampling framework <ref type="bibr" target="#b3">[Chai et al. 2000]</ref> analyzes light field rendering using signal processing techniques and shows that the Nyquist view sampling rate for light fields depends on the minimum and maximum scene depths. Furthermore, they discuss how the Nyquist view sampling rate can be lowered with more knowledge of scene geometry. <ref type="bibr" target="#b47">Zhang and Chen [2003]</ref> extend this analysis to show how non-Lambertian and occlusion effects increase the spectral support of a light field, and also propose more general view sampling lattice patterns.</p><p>Rendering algorithms based on plenoptic sampling enjoy the significant benefit of prescriptive sampling; given a new scene, it is easy to compute the required view sampling density to enable highquality renderings. Many modern light field acquisition systems have been designed based on these principles, including large-scale camera systems <ref type="bibr" target="#b29">[Overbeck et al. 2018;</ref><ref type="bibr" target="#b43">Wilburn et al. 2005]</ref> and a mobile phone app <ref type="bibr" target="#b7">[Davis et al. 2012]</ref>.</p><p>We posit that prescriptive sampling is necessary for practical and useful IBR algorithms, and we extend prior theory on plenoptic sampling to show that our deep-learning-based view synthesis strategy can significantly decrease the dense sampling requirements of traditional light field rendering. Our novel view synthesis pipeline can also be used in future light field acquisition hardware systems to reduce the number of required cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Geometry-Based View Synthesis</head><p>Many IBR algorithms attempt to leverage explicit scene geometry to synthesize new views from arbitrary unstructured sets of input views. These approaches can be meaningfully categorized as either using global or local geometry.</p><p>Techniques that use global geometry generally compute a single global mesh from a set of unstructured input images. Simply texture mapping this global mesh can be effective for constrained situations such as panoramic viewing with mostly rotational and little translational viewer movement <ref type="bibr" target="#b12">[Hedman et al. 2017;</ref><ref type="bibr" target="#b13">Hedman and Kopf 2018]</ref>, but this strategy can only simulate Lambertian materials. Surface light fields <ref type="bibr" target="#b44">[Wood et al. 2000]</ref> are able to render convincing view-dependent effects, but they require accurate geometry from dense range scans and hundreds of captured images to sample the outgoing radiance at points on an object's surface.</p><p>Many free-viewpoint IBR algorithms are based upon a strategy of locally texture mapping a global mesh. The influential viewdependent texture mapping algorithm <ref type="bibr" target="#b8">[Debevec et al. 1996]</ref> proposed an approach to render novel views by blending nearby captured views that have been reprojected using a global mesh. Work on Unstructured Lumigraph Rendering <ref type="bibr" target="#b2">[Buehler et al. 2001]</ref> focused on computing per-pixel blending weights for reprojected images and proposed a heuristic algorithm that satisfied key properties for highquality rendering. Unfortunately, it is very difficult to estimate highquality meshes whose geometric boundaries align well with edges in images, and IBR algorithms based on global geometry typically suffer from significant artifacts. State-of-the-art algorithms <ref type="bibr" target="#b14">[Hedman et al. 2018</ref><ref type="bibr" target="#b15">[Hedman et al. , 2016] ]</ref> attempt to remedy this shortcoming with complicated pipelines that involve both global mesh and local depth map estimation. However, it is difficult to precisely define view sampling requirements for robust mesh estimation, and the mesh estimation procedure typically takes multiple hours, making this strategy impractical for casual content capture scenarios.</p><p>IBR algorithms that use local geometry <ref type="bibr" target="#b4">[Chaurasia et al. 2013;</ref><ref type="bibr" target="#b6">Chen and Williams 1993;</ref><ref type="bibr" target="#b22">Kopf et al. 2013;</ref><ref type="bibr" target="#b27">McMillan and Bishop 1995;</ref><ref type="bibr" target="#b28">Ortiz-Cayon et al. 2015]</ref> avoid difficult and expensive global mesh estimation. Instead, they typically compute detailed local geometry for each input image and render novel views by reprojecting and blending nearby input images. This strategy has also been extended to simulate non-Lambertian reflectance by using a second depth layer <ref type="bibr" target="#b37">[Sinha et al. 2012]</ref>. The state-of-the-art Soft3D algorithm <ref type="bibr" target="#b30">[Penner and Zhang 2017]</ref> blends between reprojected local layered representations to render novel views, which is conceptually similar to our strategy. However, Soft3D computes each local layered representation by aggregating heuristic measures of depth uncertainty over a large neighborhood of views. We instead train a deep learning pipeline end-to-end to optimize novel view quality by predicting each of our local layered representations from a much smaller neighborhood of views. Furthermore, we directly pose our algorithm within a plenoptic sampling framework, and our analysis directly applies to the Soft3D algorithm as well. We demonstrate that the high quality of our deep learning predicted local scene representations allows us to synthesize superior renderings without requiring the aggregation of geometry estimates over large view neighborhoods, as done in Soft3D. This is especially advantageous for rendering non-Lambertian effects because the apparent depth of specularities generally varies with the observation viewpoint, so smoothing the estimated geometry over large viewpoint neighborhoods prevents accurate rendering of these effects.</p><p>Other IBR algorithms <ref type="bibr" target="#b1">[Anderson et al. 2016]</ref> have attempted to be more robust to incorrect camera poses or scene motion by interpolating views using more general 2D optical flow instead of 1D depth. Local pixel shifts are also encoded in the phase information, and algorithms have exploited this to extrapolate views from microbaseline stereo pairs <ref type="bibr" target="#b9">[Didyk et al. 2013;</ref><ref type="bibr" target="#b19">Kellnhofer et al. 2017;</ref><ref type="bibr" target="#b49">Zhang et al. 2015]</ref> without explicit flow computation. However, these methods require extremely close input views and are not suited for large baseline view interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Learning for View Synthesis</head><p>Other recent methods have trained deep learning pipelines end-toend for view synthesis. This includes recent angular superresolution methods <ref type="bibr" target="#b45">[Wu et al. 2017;</ref><ref type="bibr" target="#b46">Yeung et al. 2018]</ref> that interpolate dense views within a light field camera's aperture but cannot handle sparser input view sampling since they do not model scene geometry. The DeepStereo algorithm <ref type="bibr" target="#b10">[Flynn et al. 2016]</ref>, deep learning based light field camera view interpolation <ref type="bibr" target="#b17">[Kalantari et al. 2016]</ref>, and single view local light field synthesis <ref type="bibr" target="#b39">[Srinivasan et al. 2017]</ref> each use a deep network to predict depth separately for every novel view. However, predicting local geometry separately for each view results in inconsistent renderings across smoothly-varying viewpoints.</p><p>Finally, <ref type="bibr" target="#b50">Zhou et al. [2018]</ref> introduce a deep learning pipeline to predict an MPI from a narrow baseline stereo pair for the task of stereo magnification. As opposed to previous deep learning strategies for view synthesis, this approach enforces consistency by using the same predicted scene representation to render all novel views. We adopt MPIs as our local light field representation and introduce specific technical improvements to enable larger-baseline view interpolation from many input views, in contrast to local view extrapolation from a stereo pair using a single MPI. We predict multiple MPIs, one for each input view, and train our system end-to-end through a blending procedure to optimize the resulting MPIs to be used in concert for rendering output views. We propose a 3D convolutional neural network (CNN) architecture that dynamically adjusts the number of depth planes based on the input view sampling rate, rather than a 2D CNN with a fixed number of output planes. Additionally, we show that state-of-the-art performance requires only an easily-generated synthetic dataset and a small real fine-tuning dataset, rather than a large real dataset. This allows us to generate training data captured on 2D irregular grids similar to handheld view sampling patterns, while the YouTube dataset in <ref type="bibr" target="#b50">Zhou et al. [2018]</ref> is restricted to 1D camera paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL SAMPLING ANALYSIS</head><p>The overall strategy of our method is to use a deep learning pipeline to promote each sampled view to a layered scene representation with D depth layers, and render novel views by blending between renderings from neighboring scene representations. In this section, we show that the full set of scene representations predicted by our deep network can be interpreted as a specific form of light field sampling. We extend prior work on plenoptic sampling to show that our strategy can theoretically reduce the number of required sampled views by a factor of D 2 compared to the number required by traditional Nyquist view sampling. Section 6.1 empirically shows that we are able to take advantage of this bound to reduce the number of required views by up to 64 2 ≈ 4000×.</p><p>In the following analysis, we consider a "flatland" light field with a single spatial dimension x and view dimension u for notational clarity, but note that all findings apply to general light fields with two spatial and two view dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Nyquist Rate View Sampling</head><p>Initial work on plenoptic sampling <ref type="bibr" target="#b3">[Chai et al. 2000]</ref> derived that the Fourier support of a light field, ignoring occlusion and non-Lambertian effects, lies within a double-wedge shape whose bounds are set by the minimum and maximum scene depths z min and z max , as visualized in Figure <ref type="figure" target="#fig_1">2</ref>. <ref type="bibr" target="#b47">Zhang and Chen [2003]</ref> showed that occlusions expand the light field's Fourier support because an occluder convolves the spectrum of the light field due to farther scene content with a kernel that lies on the line corresponding to the occluder's depth. The light field's Fourier support considering occlusions is limited by the effect of the closest occluder convolving the line corresponding to the furthest scene content, resulting in the parallelogram shape illustrated in Figure <ref type="figure" target="#fig_2">3a</ref>, which can only be packed half as densely as the double-wedge. The required maximum camera sampling interval ∆ u for a light field with occlusions is:</p><formula xml:id="formula_0">∆ u ≤ 1 2K x f (1/z min -1/z max ) .<label>(1)</label></formula><p>K x is the highest spatial frequency represented in the sampled light field, determined by the highest spatial frequency in the continuous light field B x and the camera spatial resolution ∆ x :</p><formula xml:id="formula_1">K x = min B x , 1 2∆ x .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MPI Scene Representation and Rendering</head><p>The MPI scene representation <ref type="bibr" target="#b50">[Zhou et al. 2018]</ref> consists of a set of fronto-parallel RGBα planes, evenly sampled in disparity within a reference camera's view frustum (see Figure <ref type="figure" target="#fig_3">4</ref>). We can render novel views from an MPI at continuously-valued camera poses within a local neighborhood by alpha compositing the color along rays into the novel view camera using the "over" operator <ref type="bibr" target="#b31">[Porter and Duff 1984]</ref>. This rendering procedure is equivalent to reprojecting each MPI plane onto the sensor plane of the novel view camera and alpha compositing the MPI planes from back to front, as observed in early work on volume rendering <ref type="bibr" target="#b23">[Lacroute and Levoy 1994]</ref>. An MPI can be considered as an encoding of a local light field, similar to layered light field displays <ref type="bibr" target="#b41">[Wetzstein et al. 2011</ref><ref type="bibr" target="#b42">[Wetzstein et al. , 2012]]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">View Sampling Rate Reduction</head><p>Plenoptic sampling theory <ref type="bibr" target="#b3">[Chai et al. 2000]</ref> additionally shows that decomposing a scene into D depth ranges and separately sampling the light field within each range allows the camera sampling interval to be increased by a factor of D. This is because the spectrum of the light field emitted by scene content within each depth range lies within a tighter double-wedge that can be packed D times more tightly than the full scene's double-wedge spectrum. Therefore, a tighter reconstruction filter with a different shear can be used for each depth range, as illustrated in Figure <ref type="figure" target="#fig_1">2b</ref>. The reconstructed light field, ignoring occlusion effects, is simply the sum of the reconstructions of all layers, as shown in Figure <ref type="figure" target="#fig_1">2c</ref>.</p><p>However, it is not straightforward to extend this analysis to handle occlusions, because the union of the Fourier spectra for all depth ranges has a smaller support than the original light field with occlusions, as visualized in Figure <ref type="figure" target="#fig_2">3c</ref>. Instead, we observe that reconstructing a full scene light field from these depth range light fields while respecting occlusions would be much easier given corresponding per-view opacities, or shield fields <ref type="bibr" target="#b24">[Lanman et al. 2008]</ref>, for each layer. We could then easily alpha composite the depth range light fields from back to front to compute the full scene light field.</p><p>Each alpha compositing step increases the Fourier support by convolving the previously-accumulated light field's spectrum with the spectrum of the occluding depth layer. As is well known in signal processing, the convolution of two spectra has a Fourier bandwidth equal to the sum of the original spectra's bandwidths. Figure <ref type="figure" target="#fig_2">3b</ref> illustrates that the width of the Fourier support parallelogram for each depth range light field, considering occlusions, is:</p><formula xml:id="formula_2">2K x f (1/z min -1/z max ) /D,<label>(3)</label></formula><p>so the resulting reconstructed light field of the full scene will enjoy the full Fourier support width.</p><p>We apply this analysis to our algorithm by interpreting the predicted MPI layers at each camera sampling location as view samples of scene content within non-overlapping depth ranges, and noting that applying the optimal reconstruction filter <ref type="bibr" target="#b3">[Chai et al. 2000]</ref> for each depth range is equivalent to reprojecting and then blending pre-multiplied RGBα planes from neighboring MPIs. Our MPI layers differ from layered renderings considered in traditional plenoptic sampling because we predict opacities in addition to color for each layer, which allows us to correctly respect occlusions while compositing the depth layer light fields.</p><p>In summary, we extend the layered plenoptic sampling framework to correctly handle occlusions by taking advantage of our predicted opacities, and show that this still allows us to increase the required camera sampling interval by a factor of D:</p><formula xml:id="formula_3">∆ u ≤ D 2K x f (1/z min -1/z max).<label>(4)</label></formula><p>Our framework further differs from classic layered plenoptic sampling in that each MPI is sampled within a reference camera view frustum with a finite field of view, instead of the infinite field of view assumed in prior analyses <ref type="bibr" target="#b3">[Chai et al. 2000;</ref><ref type="bibr" target="#b47">Zhang and Chen 2003]</ref>. In order for the MPI prediction procedure to succeed, every point within the scene's bounding volume should fall within the frustums of at least two neighboring sampled views. The required camera sampling interval ∆ u is then additionally bounded by:</p><formula xml:id="formula_4">∆ u ≤ W ∆ x z min 2f<label>(5)</label></formula><p>where W is the image width in pixels of each sampled view. The overall camera sampling interval must satisfy both constraints:</p><formula xml:id="formula_5">∆ u ≤ min D 2K x f (1/z min -1/z max ) , W ∆ x z min 2f.<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Image Space Interpretation of View Sampling</head><p>It is useful to interpret the required camera sampling rate in terms of the maximum pixel disparity d max of any scene point between adjacent input views. If we set z max = ∞ to allow scenes with content up to an infinite depth and additionally set K x = 1/2∆ x to allow spatial frequencies up to the maximum representable frequency:</p><formula xml:id="formula_6">∆ u f ∆ x z min = d max ≤ min D, W 2 .<label>(7)</label></formula><p>Simply put, the maximum disparity of the closest scene point between adjacent views must be less than min(D,W /2) pixels. When D = 1, this inequality reduces to the Nyquist bound: a maximum of 1 pixel of disparity between views.</p><p>In summary, promoting each view sample to an MPI scene representation with D depth layers allows us to decrease the required view sampling rate by a factor of D, up to the required field of view overlap for stereo geometry estimation. Light fields for real 3D scenes must be sampled in two viewing directions, so this benefit is compounded into a sampling reduction of D 2 . Section 6.1 empirically validates that our algorithm's performance matches this theoretical analysis. Section 7.1 describes how we apply the above theory along with the empirical performance of our deep learning pipeline to prescribe practical sampling guidelines for users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PRACTICAL VIEW SYNTHESIS PIPELINE</head><p>We present a practical and robust method for synthesizing new views from a set of input images and their camera poses. Our method first uses a CNN to promote each captured input image to an MPI, then reconstructs novel views by blending renderings from nearby MPIs. Figures <ref type="figure" target="fig_0">1</ref> and <ref type="figure" target="#fig_4">5</ref> visualize this pipeline. We discuss the practical image capture process enabled by our method in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MPI Prediction for Local Light Field Expansion</head><p>The first step in our pipeline is expanding each sampled view to a local light field using an MPI scene representation. Our MPI prediction pipeline takes five views as input: the reference view to be expanded and its four nearest neighbors in 3D space. Each image is reprojected to D depth planes, sampled linearly in disparity within the reference view frustum, to form 5 plane sweep volumes (PSVs) of size H × W × D × 3.</p><p>Our 3D CNN takes these 5 PSVs as input, concatenated along the channel dimension. This CNN outputs an opacity α for each MPI coordinate (x, y, d) as well as a set of 5 color selection weights that sum to 1 at each MPI coordinate. These weights parameterize the RGB values in the output MPI as a weighted combination of the input PSVs. Intuitively, each predicted MPI softly "selects" its color values at each MPI coordinate from the pixel colors at that coordinate in each of the input PSVs. We specifically use this RGB parameterization instead of the foreground+background parameterization proposed by <ref type="bibr" target="#b50">Zhou et al. [2018]</ref> because their method does not allow an MPI to directly incorporate content occluded from the reference view but visible in other input views.</p><p>Furthermore, we enhance the MPI prediction CNN architecture from the original version to use 3D convolutional layers instead of the original 2D convolutional layers so that our architecture is fully convolutional along the height, width, and depth dimensions. This enables us to predict MPIs with a variable number of planes D so that we can jointly choose the view and disparity sampling densities to satisfy Equation <ref type="formula" target="#formula_6">7</ref>. Table <ref type="table" target="#tab_1">2</ref> validates the benefit of being able to change the number of MPI planes to correctly match our derived sampling requirements, enabled by our use of 3D convolutions. Our full network architecture can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Continuous View Reconstruction by Blending</head><p>As discussed in Section 3, we reconstruct interpolated views as a weighted combination of renderings from multiple nearby MPIs. This effectively combines our local light field approximations into a light field with a near plane spanning the extent of the captured input views and a far plane determined by the field-of-view of the input views. As in standard light field rendering, this allows for a new view path with unconstrained 3D translation and rotation within the range of views made up of rays in the light field.</p><p>One important detail in our rendering process is that we consider the accumulated alpha values from each MPI rendering when blending. This allows each MPI rendering to "fill in" content that is occluded from other camera views.</p><p>Our MPI prediction network uses a set of RGB images C k along with their camera poses p k to produce a set of MPIs M k (one corresponding to each input image). To render a novel view with pose p t using the predicted MPI M k , we homography warp each RGBα MPI plane into the frame of reference of the target pose p t then alpha composite the warped planes together from back to front. This produces an RGB image and an alpha image, which we denote C t,k and α t,k respectively (subscript t, k indicating that the output is rendered at pose p t using the MPI at pose p k ).</p><p>Since a single MPI alone will not necessarily contain all the content visible from the new camera pose due to occlusions and field of view issues, we generate the final RGB output C t by blending rendered RGB images C t,k from multiple MPIs, as depicted in Figure <ref type="figure" target="#fig_4">5</ref>. We use scalar blending weights w t,k , each modulated by the corresponding accumulated alpha images α t,k and normalized so that the resulting rendered image is fully opaque (α = 1):</p><formula xml:id="formula_7">C t = k w t,k α t,k C t,k k w t,k α t,k .<label>(8)</label></formula><p>For an example where modulating the blending weights by the accumulated alpha values prevents artifacts in C t , see Figure <ref type="figure" target="#fig_5">6</ref>. Table <ref type="table" target="#tab_1">2</ref> demonstrates that blending with alpha gives quantitatively superior results over both using a single MPI and blending multiple MPI renderings without using the accumulated alpha.</p><p>The blending weights w t,k can be any sufficiently smooth filter. In the case of data sampled on a regular grid, we use bilinear interpolation from the four nearest MPIs rather than the ideal sinc function interpolation for effiency and due to the limited number of sampled views. For irregularly sampled data, we use the five nearest MPIs and take w t,k ∝ exp (-γ ℓ(p t , p k )). Here ℓ(p t , p k ) is the L 2 distance between the translation vectors of poses p t and p k , and the constant γ is defined as f Dz min given focal length f , minimum distance to the scene z min , and number of planes D. (Note that the quantity f ℓ z min represents ℓ converted into units of pixel disparity.)</p><p>Our strategy of blending between neighboring MPIs is particularly effective for rendering non-Lambertian effects. For general   curved surfaces, the virtual apparent depth of a specularity changes with the viewpoint <ref type="bibr" target="#b40">[Swaminathan et al. 2002]</ref>. As a result, specularities appear as curves in epipolar slices of the light field, while diffuse points appear as lines. Each of our predicted MPIs can represent a specularity for a local range of views by placing the specularity at a single virtual depth. Figure <ref type="figure" target="#fig_6">7</ref> illustrates how our rendering procedure effectively models a specularity's curve in the light field by blending locally linear approximations, as opposed to the limited extrapolation provided by a single MPI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TRAINING OUR VIEW SYNTHESIS PIPELINE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Dataset</head><p>We train our view synthesis pipeline using both renderings and real images of natural scenes. Using synthetic training data crucially enables us to easily generate a large dataset with input view and scene depth distributions similar to those we expect at test time, while using real data helps us generalize to real-world lighting and reflectance effects as well as small errors in pose estimation.</p><p>Our synthetic training set consists of images rendered from the SUNCG <ref type="bibr" target="#b38">[Song et al. 2017]</ref> and UnrealCV <ref type="bibr" target="#b32">[Qiu et al. 2017]</ref> datasets. SUNCG contains 45,000 simplistic house and room environments with texture mapped surfaces and low geometric complexity. Un-realCV contains only a few large scale environments, but they are modeled and rendered with extreme detail, providing geometric complexity, texture variety, and non-Lambertian reflectance effects. We generate views for each synthetic training instance by first randomly sampling a target baseline for the inputs (up to 128 pixels of disparity), then randomly perturbing the camera pose in 3D to approximately match this baseline.</p><p>Our real training dataset consists of 24 scenes from our handheld cellphone captures, with 20-30 images each. We use the COLMAP structure from motion <ref type="bibr" target="#b33">[Schönberger and Frahm 2016]</ref> implementation to compute poses for our real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Procedure</head><p>For each training step, we sample two sets of 5 views each to use as inputs, and a single held-out target view for supervision. We first use the MPI prediction network to predict two MPIs, one from each set of 5 inputs. Next, we render the target novel view from both MPIs and blend these renderings using the accumulated alpha values, as described in Equation <ref type="formula" target="#formula_7">8</ref>.</p><p>The training loss is simply the image reconstruction loss for the rendered novel view. We follow the original work on MPI prediction <ref type="bibr" target="#b50">[Zhou et al. 2018]</ref> and use a VGG network activation perceptual loss as implemented by <ref type="bibr" target="#b5">Chen and Koltun [2017]</ref>, which has been consistently shown to outperform standard image reconstruction losses <ref type="bibr" target="#b16">[Huang et al. 2018;</ref><ref type="bibr" target="#b48">Zhang et al. 2018]</ref>. We are able to supervise only the final blended rendering because both our fixed rendering and blending functions are differentiable. Learning through this blending step trains our MPI prediction network to leave alpha "holes" in uncertain regions for each MPI, in the expectation that this content will be correctly rendered by another neighboring MPI, as illustrated by Figure <ref type="figure" target="#fig_5">6</ref>.</p><p>In practice, training through blending is slower than training a single MPI, so we first train the network to render a new view from only one MPI for 500k iterations, then train the full pipeline (blending views from two different MPIs) for 100k iterations. To fine tune the network to process real data, we train on our small real dataset for an additional 10k iterations. We use 320 × 240 resolution and up to 128 planes for SUNCG training data, and 640 × 480 resolution and up to 32 planes for UnrealCV training data, due to GPU memory limitations. We implement our full pipeline in Tensorflow <ref type="bibr" target="#b0">[Abadi et al. 2015]</ref> and optimize the MPI prediction network parameters using Adam <ref type="bibr" target="#b21">[Kingma and Ba 2015]</ref> with a learning rate of 2 × 10 -4 and a batch size of one. We split the training pipeline across two Nvidia RTX 2080Ti GPUs, using one GPU to generate each MPI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL EVALUATION</head><p>We quantitatively and qualitatively validate our method's prescriptive sampling benefits and ability to render high fidelity novel views of light fields that have been undersampled by up to 4000×, as well as demonstrate that our algorithm outperforms state-of-the-art methods for regular view interpolation. Figure <ref type="figure" target="#fig_8">9</ref> showcases these qualitative comparisons on scenes with complex geometry (Fern and T-Rex) and highly non-Lambertian scenes (Air Plants and Pond) that are not handled well by most view synthesis algorithms.</p><p>For all quantitative comparisons (Table <ref type="table" target="#tab_1">2</ref>), we use a synthetic test set rendered from an UnrealCV <ref type="bibr" target="#b32">[Qiu et al. 2017]</ref> environment that was not used to generate any training data. Our test set contains 8 scenes, each rendered at 640 × 480 resolution and at 8 different view sampling densities such that the maximum disparity between adjacent input views ranges from 1 to 256 pixels (a maximum disparity of 1 pixel between input views corresponds to Nyquist rate view sampling). We restrict our quantitative comparisons to rendered images because a Nyquist rate grid-sampled light field would require at least 384 2 camera views to generate a similar test set, and no such densely-sampled real light field dataset exists to the best of our knowledge. We report quantitative performance using the standard PSNR and SSIM metrics, as well as the state-of-the-art LPIPS <ref type="bibr" target="#b48">[Zhang et al. 2018]</ref> perceptual metric, which is based on a weighted combination of neural network activations tuned to match human judgements of image similarity.</p><p>Finally, our accompanying video shows results on over 60 additional real-world scenes. These renderings were created completely automatically by a script that takes only the set of captured images and desired output view path as inputs, highlighting the practicality and robustness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sampling Theory Validation</head><p>Our method is able to render high-quality novel views while significantly decreasing the required input view sampling density compared to standard light field interpolation. Figure <ref type="figure" target="#fig_7">8</ref> shows that our method is able to render novel views with Nyquist level perceptual quality with up to d max = 64 pixels of disparity between input view samples, as long as we match the number of planes in each MPI to the maximum pixel disparity between input views. We postulate that our inability to match Nyquist quality from input images with a maximum of 128 pixels of disparity is due to the effect of occlusions. It becomes increasingly likely that any non-foreground scene point will be sampled by fewer input views as the maximum disparity between adjacent views increases. This increases the difficulty of depth estimation and requires the CNN to hallucinate the appearance and depth of occluded points in extreme cases where they are sampled by none of the input views.</p><p>Figure <ref type="figure" target="#fig_7">8</ref> also shows that once our sampling bound is satisfied, adding additional planes does not increase performance. For example, at 32 pixels of disparity, increasing from 8 to 16 to 32 planes decreases the LPIPS error, but performance stays constant from 32 to 128 planes. This verifies that for scenes up to 64 pixels of disparity, adding additional planes past the maximum pixel disparity between input views is of limited value, in accordance with our theoretical claim that partitioning a scene with disparity variation of D pixels into D depth ranges is sufficient for continuous reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparisons to Baseline Methods</head><p>We quantitatively (Table <ref type="table" target="#tab_1">2</ref>) and qualitatively (Figure <ref type="figure" target="#fig_8">9</ref>) demonstrate that our algorithm produces superior renderings, particularly for non-Lambertian effects, without the artifacts seen in renderings from competing methods. We urge readers to view our accompanying video for convincing rendered camera paths that highlight the benefits of our approach.</p><p>We compare our method to state-of-the-art view synthesis techniques as well as view-dependent texture mapping using a global mesh as proxy geometry. Please refer to Appendix A for additional implementation details regarding baseline methods.</p><p>Light Field Interpolation (LFI) <ref type="bibr" target="#b3">[Chai et al. 2000]</ref>. This baseline is representative of continuous view reconstruction based on classic signal processing. Following the method of plenoptic sampling <ref type="bibr" target="#b3">[Chai et al. 2000]</ref>, we render novel views using a bilinear interpolation reconstruction filter sheared to the mean scene disparity. Figure <ref type="figure" target="#fig_8">9</ref> demonstrates that increasing the camera spacing beyond the Nyquist rate results in aliasing and ghosting artifacts when using this method.</p><p>Unstructured Lumigraph Rendering (ULR) <ref type="bibr" target="#b2">[Buehler et al. 2001]</ref>. This baseline is representative of view dependent texture mapping with an estimated global mesh as a geometry proxy. We reconstruct a global mesh from all inputs using the screened Poisson surface reconstruction algorithm <ref type="bibr" target="#b18">[Kazhdan and Hoppe 2013]</ref>, and use the heuristic Unstructured Lumigraph blending weights <ref type="bibr" target="#b2">[Buehler et al. 2001]</ref> to blend input images after reprojecting them into the novel viewpoint using the global mesh. We use a plane at the mean scene disparity as a proxy geometry to fill in holes in the mesh.</p><p>It is particularly difficult to reconstruct a global mesh with geometry edges that are well-aligned with image edges, which causes perceptually jarring artifacts. Furthermore, mesh reconstruction often fails to fill in large portions of the scene, resulting in ghosting artifacts similar to those seen in light field interpolation.</p><p>Soft3D <ref type="bibr" target="#b30">[Penner and Zhang 2017]</ref>. Soft3D is a state-of-the-art view synthesis algorithm that is similar to our approach in that it also computes a local layered scene representation for each input view and projects and blends these volumes to render each novel view. However, it uses a hand-crafted pipeline based on classic local stereo and guided filtering to compute each layered representation. Furthermore, since classic stereo methods are unreliable for smooth or repetitive image textures and non-Lambertian materials, Soft3D relies on smoothing their geometry estimation across many (up to 25) input views.</p><p>Table <ref type="table" target="#tab_1">2</ref> quantitatively demonstrates that our approach outperforms Soft3D overall. In particular, Soft3D's performance degrades much more rapidly as the input view sampling rate decreases since their aggregation is less effective when fewer input images view the same scene content. Our method is able to predict high-quality geometry in scenarios where Soft3D suffers from noisy and erroneous results of local stereo because we leverage deep learning to learn implicit priors on natural scene geometry. This is in line with recent work that has shown the benefits of deep learning over traditional stereo for depth estimation <ref type="bibr" target="#b16">[Huang et al. 2018;</ref><ref type="bibr" target="#b20">Kendall et al. 2017]</ref>.</p><p>Figure <ref type="figure" target="#fig_8">9</ref> qualitatively demonstrates that Soft3D generally contains blurred geometry artifacts due to errors in local depth estimation, and that Soft3D's approach fails for rendering non-Lambertian effects because their aggregation procedure blurs the specularity geometry, which changes with the input image viewpoint.</p><p>Backwards warping deep network (BW Deep). This baseline subsumes recent deep learning view synthesis techniques <ref type="bibr" target="#b10">[Flynn et al. 2016;</ref><ref type="bibr" target="#b17">Kalantari et al. 2016]</ref>, which use a CNN to estimate geometry for each novel view and then backwards warp and blend nearby input images to render the target view. We train a network that uses the same 3D CNN architecture as our MPI prediction network but instead outputs a single depth map at the pose of the new target view. We then backwards warp the five input images into the new view using this depth map and use a second 2D CNN to composite these warped input images into a single rendered output view. As shown in Table <ref type="table" target="#tab_1">2</ref>, performance for this method degrades quickly as the maximum disparity increases. Although this approach produces comparable images to our method for scenes with small disparities (d max = 16, 32), the renderings suffer from extreme inconsistency when rendering video sequences.</p><p>BW Deep methods use a CNN to estimate depth separately for each output viewpoint, so artifacts appear and disappear over only a few frames, resulting in rapid flickers and pops in the rendered camera path. This inconsistency is visible as corruption in the epipolar plots in Figure <ref type="figure" target="#fig_8">9</ref> and can be clearly seen in our supplemental video. Furthermore, backwards warping incentivizes incorrect depth predictions to fill in disocclusions, so BW Deep methods also produce errors around thin structures and occlusion edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies</head><p>We validate our overall strategy of blending between multiple MPIs as well as our specific blending procedure using accumulated alphas with the following ablation studies:</p><p>Single MPI. The fifth row of Table <ref type="table" target="#tab_1">2</ref> shows that using only one MPI to produce new views results in significantly decreased performance due to the limited field of view represented in a single MPI as well as depth discretization artifacts as the target view moves far from the MPI reference viewpoint. Additionally, Figure <ref type="figure" target="#fig_6">7</ref> shows an example of complex non-Lambertian reflectance that cannot be represented by a single MPI. This ablation can be considered an upper bound on the performance of <ref type="bibr" target="#b50">Zhou et al. [2018]</ref>, since we use one MPI generated by a higher capacity 3D CNN.</p><p>Average MPIs. The sixth row of Table <ref type="table" target="#tab_1">2</ref> shows that blending multiple MPI outputs for each novel view without using the accumulated alpha channels results in decreased performance. Figure <ref type="figure" target="#fig_5">6</ref> visualizes that this simple blending leads to ghosting in regions that are occluded from the poses of any of the MPIs used for rendering, because they will contain incorrect content in disoccluded regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">PRACTICAL USAGE</head><p>We present guidelines to assist users in sampling views that enable high-quality view interpolation with our algorithm, and showcase our method's practicality with a smartphone camera app that guides users to easily capture such input images. Furthermore, we implement a mobile viewer that renders novel views from our predicted MPIs in real-time. Figure <ref type="figure" target="#fig_8">9</ref> showcases examples of rendered results from handheld smartphone captures. Our accompanying video contains a screen capture of our app in use, as well as results on over 60 real-world scenes generated by an automated script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Prescriptive Scene Sampling Guidelines</head><p>In a typical capture scenario, a user will have a camera with a field of view θ and a world space plane with side length S that bounds the viewpoints they wish to render. Based on this, we prescribe the design space of image resolution W and number of images to sample N that users can select from to reliably render novel views at Nyquist-level perceptual quality.</p><p>Section 6.1 shows that the empirical limit on the maximum disparity d max between adjacent input views for our deep learning pipeline is 64 pixels. Substituting Equation <ref type="formula" target="#formula_6">7</ref>:</p><formula xml:id="formula_8">∆ u f ∆ x z min ≤ 64.<label>(9)</label></formula><p>We translate this into user-friendly quantities by noting that ∆ u = S/ √ N and that the ratio of sensor width to focal length W ∆ x /f = 2 tan θ /2:</p><formula xml:id="formula_9">W √ N ≤ 128z min tan(θ /2) S .<label>(10)</label></formula><p>Using a smartphone camera with a 64 • field of view, this is simply:</p><formula xml:id="formula_10">W √ N ≤ 80z min S .<label>(11)</label></formula><p>Intuitively, once a user has determined the extent of viewpoints they wish to render and the depth of the closest scene point, they can choose any target rendering resolution W and number of images to capture N such that the ratio W / √ N satisfies the above expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Asymptotic Rendering Time and Space Complexity</head><p>Within the possible choices of rendering resolution W and number of sampled views N that satisfy the above guideline, different users may value capture time, rendering time, and storage costs differently. We derive the asymptotic complexities of these quantities to further assist users in choosing correct parameters for their application.</p><p>First, the capture time is simply O(N ). The render time of each MPI generated is proportional to the number of planes times the pixels per plane:</p><formula xml:id="formula_11">W 2 D = W 3 S 2 √ N z min tan(θ /2) = O(W 3 N -1/2 ).<label>(12)</label></formula><p>Note that the rendering time for each MPI decreases as the number of sampled images N increases, because this allows us to use fewer planes per MPI. The total MPI storage cost is proportional to:</p><formula xml:id="formula_12">W 2 D • N = W 3 S √ N min tan(θ /2) = O(W 3 N 1/2 ).<label>(13)</label></formula><p>Practically, this means that users should determine their specific rendering time and storage constraints, and then maximize the image resolution and number of sampled views that satisfy their constraints as well as the guideline in Equation <ref type="formula" target="#formula_9">10</ref>. Figure <ref type="figure" target="#fig_9">10</ref> visualizes these constraints for an example user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Smartphone Capture App</head><p>We develop an app for iOS smartphones, based on the ARKit framework, that guides users to capture input views for our view synthesis algorithm. The user first taps the screen to mark the closest object, and the app uses the corresponding scene depth computed by ARKit as z min . Next, the user selects the size of the view plane S within which our algorithm will render novel views. We fix the rendering resolution for the smartphone app to W = 500 which therefore fixes the prescribed number and spacing of required images based on Equation <ref type="formula" target="#formula_10">11</ref> and the definition ∆ u = S/ √ N . Our app then guides the user to capture these views using the intuitive augmented reality overlay shown in Figure <ref type="figure" target="#fig_10">11</ref>. When the phone detects that the camera has been moved to a new sample location, it automatically records an image and highlights the next sampling point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Preprocessing</head><p>After capturing the required input images, the only preprocessing required before being able to render novel views is estimating the input camera poses and using our trained network to predict an MPI for each input view. Unfortunately, camera poses from ARKit are currently not accurate enough for acceptable results, so we use the open source COLMAP software package <ref type="bibr" target="#b33">[Schönberger and Frahm 2016;</ref><ref type="bibr" target="#b34">Schönberger et al. 2016]</ref>, which takes about 2-6 minutes for sets of 20-30 input images.</p><p>We use the deep learning pipeline described in Section 4.1 to predict an MPI for each input sampled view. On an Nvidia GTX 1080Ti GPU, This takes approximately 0.5 seconds for a small MPI (500 × 350 × 32 ≈ 6 megavoxels) or 12 seconds for a larger MPI that must be output in overlapping patches (1000 × 700 64 ≈ 45 megavoxels). In total, our method only requires about 10 minutes of preprocessing to estimate poses and predict MPIs before being able to render novel views at a 1 megapixel image resolution.</p><p>With the increasing investment in smartphone AR and on-device deep learning accelerators, we expect that smartphone pose estimation will soon be accurate enough and on-device network inference will be powerful enough for users to go from capturing images to rendering novel views within a few seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Real-Time Viewers</head><p>We implement novel view rendering from a single MPI by rasterizing each plane from back to front using texture mapped rectangles in 3D space, invoking a standard shader API to correctly handle the alpha compositing, perspective projection, and texture resampling. For each new view, we determine the MPIs to be blended, as discussed in Section 4.2, and render them into separate framebuffers. We then use a simple fragment shader to perform the alpha-weighted blending described in Section 4.2. We implement this rendering pipeline as desktop viewer using OpenGL which renders views with 1000 × 700 resolution at 60 frames per second, as well as an iOS mobile viewer using the Metal API which renders views with 500 × 350 resolution at 30 frames per second. Please see our video for demonstrations of these real-time rendering implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Limitations</head><p>A main limitation of our algorithm is that the MPI network sometimes assigns high opacity to incorrect layers in regions of ambiguous or repetitive texture and regions where scene content moves between input images. This can cause floating or blurred patches in the rendered output sequence (see the far right side of the fern in our video), which is a common failure mode in methods that rely on texture matching cues to infer depth. These artifacts could potentially be ameliorated by using more input views to disambiguate stereo matching and by encouraging the network to learn stronger global priors on 3D geometry.</p><p>Another limitation is the difficulty of scaling to higher image resolutions. As evident in Equations <ref type="formula" target="#formula_11">12</ref> and <ref type="formula" target="#formula_12">13</ref>, layered approaches such as our method are limited by complexities that scale cubically with the image width in pixels. Furthermore, increasing the image resolution requires a CNN with a larger receptive field. This could be addressed by exploring multiresolution CNN architectures and hierarchical volume representations such as octrees, or by predicting a more compact local scene representation such as layered depth images <ref type="bibr" target="#b35">[Shade et al. 1998]</ref> with opacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We have presented a simple and practical method for view synthesis that works reliably for complex real-world scenes, including non-Lambertian materials. Our algorithm first promotes each input image into a layered local light field representation, then renders novel views in real time by blending outputs generated by nearby representations. We extend traditional layered plenoptic sampling analysis to handle occlusions and provide a theoretical sampling bound on how many views are needed for our method to render high-fidelity views of a given scene. We quantitatively validate this bound and demonstrate that we match the perceptual quality of dense Nyquist rate view sampling while using ≈ 4000× fewer input images. Our accompanying video demonstrates that we thoroughly outperform prior work, and showcases results on over 60 diverse and complex real-world scenes, where our novel views are rendered with a fully automated capture-to-render pipeline. We believe that our work paves the way for future advances in image-based rendering that combine the empirical performance benefits of data-driven machine learning methods with the robust reliability guarantees of traditional geometric and signal processing based analysis.</p></div>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1.</head><label>1</label><figDesc>Fig 1. We present a simple and reliable method for view synthesis from a set of input images captured by a handheld camera in an irregular grid pattern. We theoretically and empirically demonstrate that our method enjoys a prescriptive sampling rate that requires 4000× fewer input views than Nyquist for high-fidelity view synthesis of natural scenes. Specifically, we show that this rate can be interpreted as a requirement on the pixel-space disparity of the closest object to the camera between captured views (Section 3). After capture, we expand all sampled views into layered representations that can render high-quality local light fields. We then blend together renderings from adjacent local light fields to synthesize dense paths of new views (Section 4). Our rendering consists of simple and fast computations (homography warping and alpha compositing) that can generate new views in real-time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Traditional plenoptic sampling without occlusions, as derived in <ref type="bibr" target="#b3">[Chai et al. 2000]</ref>. (a) The Fourier support of a light field without occlusions lies within a double-wedge, shown in blue. Nyquist rate view sampling is set by the double-wedge width, which is determined by the minimum and maximum scene depths [z min , z max ] and the maximum spatial frequency K x . The ideal reconstruction filter is shown in orange. (b) Splitting the light field into D non-overlapping layers with equal disparity width decreases the Nyquist rate by a factor of D. (c) Without occlusions, the full light field spectrum is the sum of the spectra from each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. We extend traditional plenoptic sampling to consider occlusions when reconstructing a continuous light field from MPIs. (a) Considering occlusions expands the Fourier support to a parallelogram (the Fourier support without occlusions is shown in blue and occlusions expand the Fourier support to additionally include the purple region) and doubles the Nyquist view sampling rate. (b) As in the no-occlusions case, separately reconstructing the light field for D layers decreases the Nyquist rate by a factor of D. (c) With occlusions, the full light field spectrum cannot be reconstructed by summing the individual layer spectra because the union of their supports is smaller than the support of the full light field spectrum (a). Instead, we compute the full light field by alpha compositing the individual light field layers from back to front in the primal domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. We promote each input view sample to an MPI scene representation<ref type="bibr" target="#b50">[Zhou et al. 2018]</ref>, consisting of D RGBα planes at regularly sampled disparities within the input view's camera frustum. Each MPI can render continuously-valued novel views within a local neighborhood by alpha compositing color along rays into the novel view's camera.</figDesc><graphic type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. We render novel views as a weighted combination of renderings from neighboring MPIs, modulated by the corresponding accumulated alphas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. An example illustrating the benefits of using accumulated alpha to blend MPI renderings. We render two MPIs at the same new camera pose. In the top row, we display the RGB outputs C t,i from each MPI as well as the accumulated alphas α t,i , normalized so that they sum to one at each pixel. In the bottom row, we see that a simple average of the RGB images C t, i retains the stretching artifacts from both MPI renderings, whereas the alpha weighted blending combines only the non-occluded pixels from each input to produce a clean output C t .</figDesc><graphic type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. We demonstrate that a collection of MPIs can approximate a highly non-Lambertian light field. In this synthetic scene, the curved plate reflects the paintings on the wall, leading to quickly-varying specularities as the camera moves horizontally. This effect can be observed in the ground truth epipolar plot (bottom right). A single MPI (top right) can only place a specular reflection at a single virtual depth, but blending renderings from multiple MPIs (middle right) provides a much better approximation to the true light field. In this example, we blend between MPIs evenly distributed at every 32 pixels of disparity along a horizontal path, indicated by the dashed lines in the epipolar plot.</figDesc><graphic type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. We plot the performance of our method (with varying number of planes D = 8, 16, 32, 64, and 128) compared to light field interpolation for different input view sampling rates (denoted by maximum scene disparity d max between adjacent input views). Our method can achieve the same perceptual quality as LFI with Nyquist rate sampling (black dotted line) as long as the number of predicted planes matches or exceeds the undersampling rate, up to an undersampling rate of 128. At D = 64, this means we achieve the same quality as LFI with 64 2 ≈ 4000× fewer views. We use the LPIPS<ref type="bibr" target="#b48">[Zhang et al. 2018]</ref> metric (lower is better) because we primarily value perceptual quality. The colored dots indicate the point on each line where the number of planes equals the maximum scene disparity, where equality is achieved in our sampling bound (Equation <ref type="formula" target="formula_6">7</ref>). The shaded region indicates ±1 standard deviation over all 8 test scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Results on real cellphone datasets. We render a sequence of new views and show both a crop from a single rendered output and an epipolar slice of the sequence. We show 2D projections of the input camera poses (blue dots) and new view path (red line) along the z and y axes of the new view camera in the lower left of each row. LFI fails to cleanly represent objects at different depths because it only uses a single depth plane for reprojection, leading to ghosting (leaves in Fern, lily pads in Pond) and depth inconsistency visible in all epipolar images. Mesh reconstruction failures cause artifacts visible in both the crops and epipolar images for ULR. Soft3D's depth uncertainty leads to blur, and geometry aggregation across large view neighborhoods results in incorrect specularity geometry (brown and blue reflections in Pond). BW Deep's use of a CNN to render every novel view causes depth inconsistency, visible as choppiness across the rows of the epipolar images in all examples. Additionally, BW Deep selects a single depth per pixel, leading to errors for transparencies (glass rim in Air Plants) and reflections (Pond). BW Deep also uses backwards warping, which causes errors around occlusion boundaries (thin ribs in T-Rex). We urge the reader to refer to our supplemental video for high quality videos of these rendered camera paths and additional discussion.</figDesc><graphic type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Time and storage cost tradeoff within the space of rendering resolution and number of sampled views that result in Nyquist level perceptual quality (space above the thick blue curve signifying D = d max ≤ 64, as in Equation <ref type="formula" target="formula_10">11</ref>). We plot isocontours of rendering time and storage space for an example scene with close depth z min = 1.0m and target view plane with side length 0.5m, captured with a camera with a 64 • field of view. We use the average rendering speed from our desktop viewer and the storage requirement from uncompressed 8-bit MPIs. Users can select the point where their desired rendering speed and storage space isocontours intersect to determine the minimum required number of views and maximum affordable rendering resolution.</figDesc><graphic type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. Equation <ref type="formula" target="formula_6">7</ref> prescribes a simple sampling bound related only to the maximum scene disparity. We take advantage of the augmented reality toolkits available in modern smartphones to create an app that helps the user sample a real scene for rendering with our method. (a) We use built-in software to track the phone's position and orientation, providing sampling guides that allow the user to space photos evenly at the target disparity. (b) Once the user has centered the phone so that the RGB axes align with one of the guides, the app automatically captures a photo.</figDesc><graphic type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Reference for symbols used in Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>We quantitatively show that our method outperforms state-of-the-art baselines and specific ablations of our method, across a wide range of input sampling rates (measured by the maximum pixel disparity d max between adjacent input views), on a synthetic test set. We display results using the standard PSNR and SSIM metrics (higher is better) as well as the LPIPS perceptual metric <ref type="bibr" target="#b48">[Zhang et al. 2018]</ref> (lower is better). The best measurement in each column is bolded. See Sections 6.2 and 6.3 for details on each comparison.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank the SIGGRAPH reviewers for their constructive comments. The technical video was created with help from <rs type="person">Julius Santiago</rs>, <rs type="person">Milos Vlaski</rs>, <rs type="person">Endre Ajandi</rs>, and <rs type="person">Christopher Schnese</rs>. The augmented reality app was developed by <rs type="person">Alex Trevor</rs>.</p><p>Ben Mildenhall is funded by a <rs type="funder">Hertz Foundation</rs> Fellowship. Pratul P. Srinivasan is funded by an <rs type="funder">NSF</rs> <rs type="grantName">Graduate Fellowship</rs>. <rs type="person">Ravi Ramamoorthi</rs> is supported in part by <rs type="funder">NSF</rs> grant <rs type="grantNumber">1617234</rs>, <rs type="funder">ONR</rs> grant <rs type="grantNumber">N000141712687</rs>, and <rs type="funder">Google Research Awards</rs>. <rs type="person">Ren Ng</rs> is supported in part by <rs type="funder">NSF</rs> grant <rs type="grantNumber">1617794</rs> and an <rs type="funder">Alfred P. Sloan Foundation</rs> Fellowship.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PSkH5bh">
					<orgName type="grant-name">Graduate Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_jSmPMHh">
					<idno type="grant-number">1617234</idno>
				</org>
				<org type="funding" xml:id="_UEjfQgs">
					<idno type="grant-number">N000141712687</idno>
				</org>
				<org type="funding" xml:id="_cTZnh9a">
					<idno type="grant-number">1617794</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A BASELINE METHODS IMPLEMENTATION DETAILS</head><p>Here we provide additional implementation details regarding the baseline comparison methods described in Section 6.2. Soft3D <ref type="bibr" target="#b30">[Penner and Zhang 2017]</ref>. We implemented this algorithm from the description provided in the original paper, since no opensource code is currently available. We provide Soft3D's local stereo stage with the same 5 input images used to compute each of our MPIs, for fair comparison. Next, we aggregate Soft3D's computed vote volumes across 25 viewpoints, as suggested in their paper for 2D view captures. Finally, we compute each rendered novel view using the Soft3D volumes corresponding to the same 5 viewpoints whose MPIs we blend between for our algorithm. The guided filter parameters are not specified in the original Soft3D paper, but we find that best results were obtained using a window size of 8 and ϵ = 20 for images with values in the range (0, 255). Please refer to our supplemental video to see results of our Soft3D implementation on a scene shown in the original paper's video to verify that our implementation is of similar quality.</p><p>Backwards warping deep network (BW Deep). This baseline is very similar to <ref type="bibr" target="#b17">[Kalantari et al. 2016]</ref> in overall structure. However, it uses a much larger CNN architecture (same architecture as our method) and takes five images as input rather than four. In addition, since our network is 3D rather than 2D, we can run inference with a variable number of planes in the plane sweep volumes rather than fixing D = 100. When generating our quantitative results (Table <ref type="table">2</ref>), we set D = d max , as in our method. We train this baseline with the same synthetic and real datasets as our network.</p><p>Unstructured Lumigraph Rendering (ULR) <ref type="bibr" target="#b2">[Buehler et al. 2001;</ref><ref type="bibr" target="#b11">Gortler et al. 1996]</ref>. We use COLMAP's multiview stereo implementation <ref type="bibr" target="#b33">[Schönberger and Frahm 2016;</ref><ref type="bibr" target="#b34">Schönberger et al. 2016]</ref> to generate dense depths and estimate a global triangle mesh of the scene from all input images using the screened Poisson surface reconstruction algorithm <ref type="bibr" target="#b18">[Kazhdan and Hoppe 2013]</ref>. For each pixel in a new target view, we implement the heuristic blending weights from <ref type="bibr" target="#b2">[Buehler et al. 2001]</ref> to blend input images reprojected using the global mesh geometry.</p><p>Light Field Interpolation (LFI) <ref type="bibr" target="#b3">[Chai et al. 2000]</ref>. This baseline is representative of classic signal processing based continuous view reconstruction. Following the method of plenoptic sampling <ref type="bibr" target="#b3">[Chai et al. 2000]</ref>, for data on a regular 2D grid, we render novel views using a bilinear interpolation reconstruction filter sheared to the mean scene disparity. For unstructured real data, we blend together 5 nearby views reprojected to the mean scene disparity, using the same blending weights as our own method.</p><p>Note that for methods that depend on a plane sweep volume (Soft3D, BW Deep, and all versions of our method), we set the number of depth planes used in the PSVs to d max.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B NETWORK ARCHITECTURE</head><p>Table 3 contains a detailed specification of our 3D CNN architecture. All layers except the last are followed by a ReLU nonlinearity and layer normalization <ref type="bibr" target="#b25">[Lei Ba et al. 2016]</ref>. The final layer outputs 5 channels. One channel is passed through a sigmoid to generate the output MPI's alpha channel. The other four (along with an all-zero channel) are passed through a softmax to get five blending weights for each voxel which are used to generate the output MPI's color channels, as described in Section 4.1. Restricting one of the softmax inputs to always be zero makes the function one-to-one rather than many-to-one.</p></div>
			</div>
			<div type="references">

				<listBibl>
			<biblStruct type="standard" xml:id="b0">
				<monogr>
					<author>
						<persName><forename>Martín</forename><surname>Abadi</surname></persName>
					</author>
					<author>
						<persName><forename>Ashish</forename><surname>Agarwal</surname></persName>
					</author>
					<author>
						<persName><forename>Paul</forename><surname>Barham</surname></persName>
					</author>
					<author>
						<persName><forename>Eugene</forename><surname>Brevdo</surname></persName>
					</author>
					<author>
						<persName><forename>Zhifeng</forename><surname>Chen</surname></persName>
					</author>
					<author>
						<persName><forename>Craig</forename><surname>Citro</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
					</author>
					<author>
						<persName><forename>Andy</forename><surname>Davis</surname></persName>
					</author>
					<author>
						<persName><forename>Jeffrey</forename><surname>Dean</surname></persName>
					</author>
					<author>
						<persName><forename>Matthieu</forename><surname>Devin</surname></persName>
					</author>
					<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
					<imprint>
						<date when="2015">2015</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b1">
				<analytic>
					
					<author>
						<persName><forename>Robert</forename><surname>Anderson</surname></persName>
					</author>
					<author>
						<persName><forename>David</forename><surname>Gallup</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
					</author>
					<author>
						<persName><forename>Janne</forename><surname>Kontkanen</surname></persName>
					</author>
					<author>
						<persName><forename>Noah</forename><surname>Snavely</surname></persName>
					</author>
					<author>
						<persName><forename>Carlos</forename><surname>Hernández</surname></persName>
					</author>
					<author>
						<persName><forename>Sameer</forename><surname>Agarwal</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
					</author>
					<title level="a">Jump: Virtual Reality Video</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2016">2016</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b2">
				<analytic>
					
					<author>
						<persName><forename>Chris</forename><surname>Buehler</surname></persName>
					</author>
					<author>
						<persName><forename>Michael</forename><surname>Bosse</surname></persName>
					</author>
					<author>
						<persName><forename>Leonard</forename><surname>Mcmillan</surname></persName>
					</author>
					<author>
						<persName><forename>Steven</forename><surname>Gortler</surname></persName>
					</author>
					<author>
						<persName><forename>Michael</forename><surname>Cohen</surname></persName>
					</author>
					<title level="a">Unstructured Lumigraph Rendering</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2001">2001</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b3">
				<analytic>
					
					<author>
						<persName><forename>Jin-Xiang</forename><surname>Chai</surname></persName>
					</author>
					<author>
						<persName><forename>Xin</forename><surname>Tong</surname></persName>
					</author>
					<author>
						<persName><forename>Sing-Chow</forename><surname>Chan</surname></persName>
					</author>
					<author>
						<persName><forename>Heung-Yeung</forename><surname>Shum</surname></persName>
					</author>
					<title level="a">Plenoptic Sampling</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2000">2000</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="aarticle" xml:id="b4">
				<analytic>
					
					<author>
						<persName><forename>Gaurav</forename><surname>Chaurasia</surname></persName>
					</author>
					<author>
						<persName><forename>Sylvain</forename><surname>Duchêne</surname></persName>
					</author>
					<author>
						<persName><forename>Olga</forename><surname>Sorkine-Hornung</surname></persName>
					</author>
					<author>
						<persName><forename>George</forename><surname>Drettakis</surname></persName>
					</author>
					<title level="a">Depth Synthesis and Local Warps for Plausible Image-based Navigation</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2013">2013</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b5">
				<analytic>
					
					<author>
						<persName><forename>Qifeng</forename><surname>Chen</surname></persName>
					</author>
					<author>
						<persName><forename>Vladlen</forename><surname>Koltun</surname></persName>
					</author>
					<title level="a">Photographic Image Synthesis With Cascaded Refinement Networks</title>
				</analytic>
				<monogr>
					<title level="j">ICCV</title>
					<imprint>
						<date when="2017">2017</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b6">
				<analytic>
					
					<author>
						<persName><forename>Eric</forename><surname>Shenchang</surname></persName>
					</author>
					<author>
						<persName><forename>Lance</forename><surname>Chen</surname></persName>
					</author>
					<author>
						<persName><surname>Williams</surname></persName>
					</author>
					<title level="a">View Interpolation for Image Synthesis</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="1993">1993</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b7">
				<analytic>
					
					<author>
						<persName><forename>Abe</forename><surname>Davis</surname></persName>
					</author>
					<author>
						<persName><forename>Marc</forename><surname>Levoy</surname></persName>
					</author>
					<author>
						<persName><forename>Fredo</forename><surname>Durand</surname></persName>
					</author>
					<title level="a">Unstructured Light Fields</title>
				</analytic>
				<monogr>
					<title level="j">Computer Graphics Forum</title>
					<imprint>
						<date when="2012">2012</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b8">
				<analytic>
					
					<author>
						<persName><forename>Paul</forename><surname>Debevec</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Camillo</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
					</author>
					<author>
						<persName><forename>Jitendra</forename><surname>Malik</surname></persName>
					</author>
					<title level="a">Modeling and Rendering Architecture from Photographs: A Hybrid Geometry-and Image-Based Approach</title>
				</analytic>
				
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="1996">1996</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b9">
				<analytic>
					
					<author>
						<persName><forename>Piotr</forename><surname>Didyk</surname></persName>
					</author>
					<author>
						<persName><forename>Pitchaya</forename><surname>Sitthi-Amorn</surname></persName>
					</author>
					<author>
						<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
					</author>
					<author>
						<persName><forename>Fredo</forename><surname>Durand</surname></persName>
					</author>
					<author>
						<persName><forename>Wojciech</forename><surname>Matusik</surname></persName>
					</author>
					<title level="a">3DTV at Home: Eulerian-Lagrangian Stereo-to-Multiview Conversion</title>
				</analytic>
				
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2013">2013</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b10">
				<analytic>
					
					<author>
						<persName><forename>John</forename><surname>Flynn</surname></persName>
					</author>
					<author>
						<persName><forename>Ivan</forename><surname>Neulander</surname></persName>
					</author>
					<author>
						<persName><forename>James</forename><surname>Philbin</surname></persName>
					</author>
					<author>
						<persName><forename>Noah</forename><surname>Snavely</surname></persName>
					</author>
					<title level="a">DeepStereo: Learning to Predict New Views From the World's Imagery</title>
				</analytic>
				
				<monogr>
					<title level="j">CVPR</title>
					<imprint>
						<date when="2016">2016</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b11">
				<analytic>
					<author>
						<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
					</author>
					<author>
						<persName><forename>Radek</forename><surname>Grzeszczuk</surname></persName>
					</author>
					<author>
						<persName><forename>Richard</forename><surname>Szeliski</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
					</author>
					<title level="a">The Lumigraph</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="1996">1996</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b12">
				<analytic>
					
					<author>
						<persName><forename>Peter</forename><surname>Hedman</surname></persName>
					</author>
					<author>
						<persName><forename>Suhib</forename><surname>Alsisan</surname></persName>
					</author>
					<author>
						<persName><forename>Richard</forename><surname>Szeliski</surname></persName>
					</author>
					<author>
						<persName><forename>Johannes</forename><surname>Kopf</surname></persName>
					</author>
					<title level="a">Casual 3D Photography</title>
				</analytic>
				
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2017">2017</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b13">
				<analytic>
					
					<author>
						<persName><forename>Peter</forename><surname>Hedman</surname></persName>
					</author>
					<author>
						<persName><forename>Johannes</forename><surname>Kopf</surname></persName>
					</author>
					<title level="a">Instant 3D Photography</title>
				</analytic>

				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2018">2018</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b14">
				<analytic>
					
					<author>
						<persName><forename>Peter</forename><surname>Hedman</surname></persName>
					</author>
					<author>
						<persName><forename>Julien</forename><surname>Philip</surname></persName>
					</author>
					<author>
						<persName><forename>True</forename><surname>Price</surname></persName>
					</author>
					<author>
						<persName><forename>Jan-Michael</forename><surname>Frahm</surname></persName>
					</author>
					<author>
						<persName><forename>George</forename><surname>Drettakis</surname></persName>
					</author>
					<author>
						<persName><forename>Gabriel</forename><surname>Brostow</surname></persName>
					</author>
					<title level="a">Deep Blending for Free-Viewpoint Image-Based Rendering</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2018">2018</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b15">
				<analytic>
					
					<author>
						<persName><forename>Peter</forename><surname>Hedman</surname></persName>
					</author>
					<author>
						<persName><forename>Tobias</forename><surname>Ritschel</surname></persName>
					</author>
					<author>
						<persName><forename>George</forename><surname>Drettakis</surname></persName>
					</author>
					<author>
						<persName><forename>Gabriel</forename><surname>Brostow</surname></persName>
					</author>
					<title level="a">Scalable Inside-Out Image-Based Rendering</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2016">2016</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b16">
				<analytic>
					
					<author>
						<persName><forename>Po-Han</forename><surname>Huang</surname></persName>
					</author>
					<author>
						<persName><forename>Kevin</forename><surname>Matzen</surname></persName>
					</author>
					<author>
						<persName><forename>Johannes</forename><surname>Kopf</surname></persName>
					</author>
					<author>
						<persName><forename>Narendra</forename><surname>Ahuja</surname></persName>
					</author>
					<author>
						<persName><forename>Jia-Bin</forename><surname>Huang</surname></persName>
					</author>
					<title level="a">DeepMVS: Learning Multi-View Stereopsis</title>
				</analytic>
				<monogr>
					<title level="j">CVPR</title>
					<imprint>
						<date when="2018">2018</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b17">
				<analytic>
					
					<author>
						<persName><forename>Nima</forename><surname>Khademi Kalantari</surname></persName>
					</author>
					<author>
						<persName><forename>Ting-Chun</forename><surname>Wang</surname></persName>
					</author>
					<author>
						<persName><forename>Ravi</forename><surname>Ramamoorthi</surname></persName>
					</author>
					<title level="a">Learning-Based View Synthesis for Light Field Cameras</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2016">2016</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b18">
				<analytic>
					
					<author>
						<persName><forename>Michael</forename><surname>Kazhdan</surname></persName>
					</author>
					<author>
						<persName><forename>Hugues</forename><surname>Hoppe</surname></persName>
					</author>
					<title level="a">Screened Poisson Surface Reconstruction</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2013">2013</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b19">
				<analytic>
					
					<author>
						<persName><forename>Petr</forename><surname>Kellnhofer</surname></persName>
					</author>
					<author>
						<persName><forename>Piotr</forename><surname>Didyk</surname></persName>
					</author>
					<author>
						<persName><forename>Szu-Po</forename><surname>Wang</surname></persName>
					</author>
					<author>
						<persName><forename>Pitchaya</forename><surname>Sitthi-Amorn</surname></persName>
					</author>
					<author>
						<persName><forename>William</forename><surname>Freeman</surname></persName>
					</author>
					<author>
						<persName><forename>Fredo</forename><surname>Durand</surname></persName>
					</author>
					<author>
						<persName><forename>Wojciech</forename><surname>Matusik</surname></persName>
					</author>
					<title level="a">3DTV at Home: Eulerian-Lagrangian Stereo-to-Multiview Conversion</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2017">2017</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b20">
				<analytic>
					
					<author>
						<persName><forename>Alex</forename><surname>Kendall</surname></persName>
					</author>
					<author>
						<persName><forename>Hayk</forename><surname>Martirosyan</surname></persName>
					</author>
					<author>
						<persName><forename>Saumitro</forename><surname>Dasgupta</surname></persName>
					</author>
					<author>
						<persName><forename>Peter</forename><surname>Henry</surname></persName>
					</author>
					<author>
						<persName><forename>Ryan</forename><surname>Kennedy</surname></persName>
					</author>
					<author>
						<persName><forename>Abraham</forename><surname>Bachrach</surname></persName>
					</author>
					<author>
						<persName><forename>Adam</forename><surname>Bry</surname></persName>
					</author>
					<title level="a">End-to-End Learning of Geometry and Context for Deep Stereo Regression</title>
				</analytic>
				<monogr>
					<title level="j">ICCV</title>
					<imprint>
						<date when="2017">2017</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b21">
				<analytic>
					
					<author>
						<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
					</author>
					<author>
						<persName><forename>Jimmy</forename><surname>Ba</surname></persName>
					</author>
					<title level="a">Adam: A Method for Stochastic Optimization</title>
				</analytic>
				<monogr>
					<title level="j">ICLR</title>
					<imprint>
						<date when="2015">2015</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b22">
				<analytic>
					
					<author>
						<persName><forename>Johannes</forename><surname>Kopf</surname></persName>
					</author>
					<author>
						<persName><forename>Fabian</forename><surname>Langguth</surname></persName>
					</author>
					<author>
						<persName><forename>Daniel</forename><surname>Scharstein</surname></persName>
					</author>
					<author>
						<persName><forename>Richard</forename><surname>Szeliski</surname></persName>
					</author>
					<author>
						<persName><forename>Michael</forename><surname>Goesele</surname></persName>
					</author>
					<title level="a">Image-Based Rendering in the Gradient Domain</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2013">2013</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b23">
				<analytic>
					<author>
						<persName><forename>Philippe</forename><surname>Lacroute</surname></persName>
					</author>
					<author>
						<persName><forename>Marc</forename><surname>Levoy</surname></persName>
					</author>
					<title level="a">Fast Volume Rendering Using a Shear-Warp Factorization of the Viewing Transformation</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="1994">1994</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b24">
				<analytic>
					<author>
						<persName><forename>Douglas</forename><surname>Lanman</surname></persName>
					</author>
					<author>
						<persName><forename>Ramesh</forename><surname>Raskar</surname></persName>
					</author>
					<author>
						<persName><forename>Amit</forename><surname>Agrawal</surname></persName>
					</author>
					<author>
						<persName><forename>Gabriel</forename><surname>Taubin</surname></persName>
					</author>
					<title level="a">Shield Fields: Modeling and Capturing 3D Occluders</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2008">2008</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b25">
				<analytic>
					
					<author>
						<persName><forename>Jimmy</forename><surname>Lei Ba</surname></persName>
					</author>
					<author>
						<persName><forename>Jamie</forename><surname>Ryan Kiros</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
					</author>
					<title level="a">Layer Normalization</title>
				</analytic>
				<monogr>
					<title level="j">arXiv:1607.06450</title>
					<imprint>
						<date when="2016">2016</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b26">
				<analytic>
					
					<author>
						<persName><forename>Marc</forename><surname>Levoy</surname></persName>
					</author>
					<author>
						<persName><forename>Pat</forename><surname>Hanrahan</surname></persName>
					</author>
					<title level="a">Light Field Rendering</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="1996">1996</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b27">
				<analytic>
					
					<author>
						<persName><forename>Leonard</forename><surname>McMillan</surname></persName>
					</author>
					<author>
						<persName><forename>Gary</forename><surname>Bishop</surname></persName>
					</author>
					<title level="a">Plenoptic Modeling: An Image-Based Rendering System</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="1995">1995</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b28">
				<analytic>
					
					<author>
						<persName><forename>Rodrigo</forename><surname>Ortiz-Cayon</surname></persName>
					</author>
					<author>
						<persName><forename>Abdelaziz</forename><surname>Djelouah</surname></persName>
					</author>
					<author>
						<persName><forename>George</forename><surname>Drettakis</surname></persName>
					</author>
					<title level="a">A Bayesian Approach for Selective Image-Based Rendering using Superpixels</title>
				</analytic>
				<monogr>
					<title level="j">International Conference on 3D Vision (3DV)</title>
					<imprint>
						<date when="2015">2015</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b29">
				<analytic>
					
					<author>
						<persName><forename type="first">Ryan</forename><forename type="middle">S</forename><surname>Overbeck</surname></persName>
					</author>
					<author>
						<persName><forename>Daniel</forename><surname>Erickson</surname></persName>
					</author>
					<author>
						<persName><forename>Daniel</forename><surname>Evangelakos</surname></persName>
					</author>
					<author>
						<persName><forename>Matt</forename><surname>Pharr</surname></persName>
					</author>
					<author>
						<persName><forename>Paul</forename><surname>Debevec</surname></persName>
					</author>
					<title level="a">A System for Acquiring, Processing, and Rendering Panoramic Light Field Stills for Virtual Reality</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2018">2018</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b30">
				<analytic>
					
					<author>
						<persName><forename>Eric</forename><surname>Penner</surname></persName>
					</author>
					<author>
						<persName><forename>Li</forename><surname>Zhang</surname></persName>
					</author>
					<title level="a">Soft 3D Reconstruction for View Synthesis</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH Asia</title>
					<imprint>
						<date when="2017">2017</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b31">
				<analytic>
					
					<author>
						<persName><forename>Thomas</forename><surname>Porter</surname></persName>
					</author>
					<author>
						<persName><forename>Tom</forename><surname>Duff</surname></persName>
					</author>
					<title level="a">Compositing Digital Images</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="1984">1984</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b32">
				<analytic>
					
					<author>
						<persName><forename>Weichao</forename><surname>Qiu</surname></persName>
					</author>
					<author>
						<persName><forename>Fangwei</forename><surname>Zhong</surname></persName>
					</author>
					<author>
						<persName><forename>Yi</forename><surname>Zhang</surname></persName>
					</author>
					<author>
						<persName><forename>Siyuan</forename><surname>Qiao</surname></persName>
					</author>
					<author>
						<persName><forename>Zihao</forename><surname>Xiao</surname></persName>
					</author>
					<author>
						<persName><forename>Tae</forename><surname>Soo Kim</surname></persName>
					</author>
					<author>
						<persName><forename>Yizhou</forename><surname>Wang</surname></persName>
					</author>
					<author>
						<persName><forename>Alan</forename><surname>Yuille</surname></persName>
					</author>
					<title level="a">UnrealCV: Virtual Worlds for Computer Vision</title>
				</analytic>
				<monogr>
					<title level="j">ACM Multimedia Open Source Software Competition</title>
					<imprint>
						<date when="2017">2017</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b33">
				<analytic>
					
					<author>
						<persName><forename>Johannes</forename><surname>Lutz Schönberger</surname></persName>
					</author>
					<author>
						<persName><forename>Jan-Michael</forename><surname>Frahm</surname></persName>
					</author>
					<title level="a">Structure-from-Motion Revisited</title>
				</analytic>
				<monogr>
					<title level="j">CVPR</title>
					<imprint>
						<date when="2016">2016</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b34">
				<analytic>
					
					<author>
						<persName><forename>Johannes</forename><surname>Lutz Schönberger</surname></persName>
					</author>
					<author>
						<persName><forename>Enliang</forename><surname>Zheng</surname></persName>
					</author>
					<author>
						<persName><forename>Marc</forename><surname>Pollefeys</surname></persName>
					</author>
					<author>
						<persName><forename>Jan-Michael</forename><surname>Frahm</surname></persName>
					</author>
					<title level="a">Pixelwise View Selection for Unstructured Multi-View Stereo</title>
				</analytic>
				<monogr>
					<title level="j">ECCV</title>
					<imprint>
						<date when="2016">2016</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b35">
				<analytic>
					
					<author>
						<persName><forename>Jonathan</forename><surname>Shade</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
					</author>
					<author>
						<persName><forename>Li</forename><surname>Wei He</surname></persName>
					</author>
					<author>
						<persName><forename>Richard</forename><surname>Szeliski</surname></persName>
					</author>
					<title level="a">Layered depth images</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="1998">1998</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b36">
				<analytic>
					
					<author>
						<persName><forename>Heung-Yeung</forename><surname>Shum</surname></persName>
					</author>
					<author>
						<persName><forename>Bing</forename><surname>Sing Kang</surname></persName>
					</author>
					<title level="a">A Review of Image-Based Rendering Techniques</title>
				</analytic>
				<monogr>
					<title level="j">Proceedings of Visual Communications and Image Processing</title>
					<imprint>
						<date when="2000">2000</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b37">
				<analytic>
					
					<author>
						<persName><forename>Sudipta</forename><surname>Sinha</surname></persName>
					</author>
					<author>
						<persName><forename>Johannes</forename><surname>Kopf</surname></persName>
					</author>
					<author>
						<persName><forename>Michael</forename><surname>Goesele</surname></persName>
					</author>
					<author>
						<persName><forename>Daniel</forename><surname>Scharstein</surname></persName>
					</author>
					<author>
						<persName><forename>Richard</forename><surname>Szeliski</surname></persName>
					</author>
					<title level="a">Image-Based Rendering for Scenes with Reflections</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2012">2012</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b38">
				<analytic>
					
					<author>
						<persName><forename>Shuran</forename><surname>Song</surname></persName>
					</author>
					<author>
						<persName><forename>Fisher</forename><surname>Yu</surname></persName>
					</author>
					<author>
						<persName><forename>Andy</forename><surname>Zeng</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
					</author>
					<author>
						<persName><forename>Manolis</forename><surname>Savva</surname></persName>
					</author>
					<author>
						<persName><forename>Thomas</forename><surname>Funkhouser</surname></persName>
					</author>
					<title level="a">Semantic Scene Completion from a Single Depth Image</title>
				</analytic>
				<monogr>
					<title level="j">CVPR</title>
					<imprint>
						<date when="2017">2017</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b39">
				<analytic>
					
					<author>
						<persName><forename type="first">Pratul</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
					</author>
					<author>
						<persName><forename>Tongzhou</forename><surname>Wang</surname></persName>
					</author>
					<author>
						<persName><forename>Ashwin</forename><surname>Sreelal</surname></persName>
					</author>
					<author>
						<persName><forename>Ravi</forename><surname>Ramamoorthi</surname></persName>
					</author>
					<author>
						<persName><forename>Ren</forename><surname>Ng</surname></persName>
					</author>
					<title level="a">Learning to Synthesize a 4D RGBD Light Field from a Single Image</title>
				</analytic>
				<monogr>
					<title level="j">ICCV</title>
					<imprint>
						<date when="2017">2017</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b40">
				<analytic>
					
					<author>
						<persName><forename>Rahul</forename><surname>Swaminathan</surname></persName>
					</author>
					<author>
						<persName><forename>Sing</forename><surname>Bing Kang</surname></persName>
					</author>
					<author>
						<persName><forename>Richard</forename><surname>Szeliski</surname></persName>
					</author>
					<author>
						<persName><forename>Antonio</forename><surname>Criminisi</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Shree</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
					</author>
					<title level="a">On the Motion and Appearance of Specularities in Image Sequences</title>
				</analytic>
				<monogr>
					<title level="j">ECCV</title>
					<imprint>
						<date when="2002">2002</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b41">
				<analytic>
					
					<author>
						<persName><forename>Gordon</forename><surname>Wetzstein</surname></persName>
					</author>
					<author>
						<persName><forename>Douglas</forename><surname>Lanman</surname></persName>
					</author>
					<author>
						<persName><forename>Wolfgang</forename><surname>Heidrich</surname></persName>
					</author>
					<author>
						<persName><forename>Ramesh</forename><surname>Raskar</surname></persName>
					</author>
					<title level="a">Layered 3D: Tomographic Image Synthesis for Attenuation-based Light Field and High Dynamic Range Displays</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2011">2011</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b42">
				<analytic>
					
					<author>
						<persName><forename>Gordon</forename><surname>Wetzstein</surname></persName>
					</author>
					<author>
						<persName><forename>Douglas</forename><surname>Lanman</surname></persName>
					</author>
					<author>
						<persName><forename>Matthew</forename><surname>Hirsch</surname></persName>
					</author>
					<author>
						<persName><forename>Ramesh</forename><surname>Raskar</surname></persName>
					</author>
					<title level="a">Tensor Displays: Compressive Light Field Synthesis using Multilayer Displays with Directional Backlighting</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2012">2012</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b43">
				<analytic>
					
					<author>
						<persName><forename>Bennett</forename><surname>Wilburn</surname></persName>
					</author>
					<author>
						<persName><forename>Neel</forename><surname>Joshi</surname></persName>
					</author>
					<author>
						<persName><forename>Vaibhav</forename><surname>Vaish</surname></persName>
					</author>
					<author>
						<persName><forename>Eino-Ville</forename><surname>Talvala</surname></persName>
					</author>
					<author>
						<persName><forename>Emilio</forename><surname>Antunez</surname></persName>
					</author>
					<author>
						<persName><forename>Adam</forename><surname>Barth</surname></persName>
					</author>
					<author>
						<persName><forename>Andrew</forename><surname>Adams</surname></persName>
					</author>
					<author>
						<persName><forename>Marc</forename><surname>Levoy</surname></persName>
					</author>
					<author>
						<persName><forename>Mark</forename><surname>Horowitz</surname></persName>
					</author>
					<title level="a">High Performance Imaging Using Large Camera Arrays</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2005">2005</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b44">
				<analytic>
					
					<author>
						<persName><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Daniel</forename><forename type="middle">I</forename><surname>Azuma</surname></persName>
					</author>
					<author>
						<persName><forename>Ken</forename><surname>Aldinger</surname></persName>
					</author>
					<author>
						<persName><forename>Brian</forename><surname>Curless</surname></persName>
					</author>
					<author>
						<persName><forename>Tom</forename><surname>Duchamp</surname></persName>
					</author>
					<author>
						<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
					</author>
					<author>
						<persName><forename>Werner</forename><surname>Stuetzle</surname></persName>
					</author>
					<title level="a">Surface Light Fields for 3D Photography</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2000">2000</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b45">
				<analytic>
					
					<author>
						<persName><forename>Gaochang</forename><surname>Wu</surname></persName>
					</author>
					<author>
						<persName><forename>Mandan</forename><surname>Zhao</surname></persName>
					</author>
					<author>
						<persName><forename>Liangyong</forename><surname>Wang</surname></persName>
					</author>
					<author>
						<persName><forename>Qionghai</forename><surname>Dai</surname></persName>
					</author>
					<author>
						<persName><forename>Tianyou</forename><surname>Chai</surname></persName>
					</author>
					<author>
						<persName><forename>Yebin</forename><surname>Liu</surname></persName>
					</author>
					<title level="a">Light Field Reconstruction Using Deep Convolutional Network on EPI</title>
				</analytic>
				<monogr>
					<title level="j">CVPR</title>
					<imprint>
						<date when="2017">2017</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b46">
				<analytic>
					
					<author>
						<persName><forename>Henry</forename><surname>Wing Fung Yeung</surname></persName>
					</author>
					<author>
						<persName><forename>Junhui</forename><surname>Hou</surname></persName>
					</author>
					<author>
						<persName><forename>Jie</forename><surname>Chen</surname></persName>
					</author>
					<author>
						<persName><forename>Yuk</forename><surname>Ying Chung</surname></persName>
					</author>
					<author>
						<persName><forename>Xiaoming</forename><surname>Chen</surname></persName>
					</author>
					<author>
						<persName><forename>Yebin</forename><surname>Liu</surname></persName>
					</author>
					<title level="a">Fast Light Field Reconstruction with Deep Coarse-to-Fine Modeling of Spatial-Angular Clues</title>
				</analytic>
				<monogr>
					<title level="j">ECCV</title>
					<imprint>
						<date when="2018">2018</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b47">
				<analytic>
					
					<author>
						<persName><forename>Cha</forename><surname>Zhang</surname></persName>
					</author>
					<author>
						<persName><forename>Tsuhan</forename><surname>Chen</surname></persName>
					</author>
					<title level="a">Spectral Analysis for Sampling Image-Based Rendering Data</title>
				</analytic>
				<monogr>
					<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
					<imprint>
						<date when="2003">2003</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b48">
				<analytic>
					
					<author>
						<persName><forename>Richard</forename><surname>Zhang</surname></persName>
					</author>
					<author>
						<persName><forename>Phillip</forename><surname>Isola</surname></persName>
					</author>
					<author>
						<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
					</author>
					<author>
						<persName><forename>Eli</forename><surname>Shechtman</surname></persName>
					</author>
					<author>
						<persName><forename>Oliver</forename><surname>Wang</surname></persName>
					</author>
					<title level="a">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</title>
				</analytic>
				<monogr>
					<title level="j">CVPR</title>
					<imprint>
						<date when="2018">2018</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b49">
				<analytic>
					
					<author>
						<persName><forename>Zhoutong</forename><surname>Zhang</surname></persName>
					</author>
					<author>
						<persName><forename>Yebin</forename><surname>Liu</surname></persName>
					</author>
					<author>
						<persName><forename>Qionghai</forename><surname>Dai</surname></persName>
					</author>
					<title level="a">Light Field from Micro-Baseline Image Pair</title>
				</analytic>
				<monogr>
					<title level="j">CVPR</title>
					<imprint>
						<date when="2015">2015</date>
					</imprint>
				</monogr>
			</biblStruct>

			<biblStruct type="article" xml:id="b50">
				<analytic>
					<author>
						<persName><forename>Tinghui</forename><surname>Zhou</surname></persName>
					</author>
					<author>
						<persName><forename>Richard</forename><surname>Tucker</surname></persName>
					</author>
					<author>
						<persName><forename>John</forename><surname>Flynn</surname></persName>
					</author>
					<author>
						<persName><forename>Graham</forename><surname>Fyffe</surname></persName>
					</author>
					<author>
						<persName><forename>Noah</forename><surname>Snavely</surname></persName>
					</author>
					<title level="a">Stereo Magnification: Learning View Synthesis using Multiplane Images</title>
				</analytic>
				<monogr>
					<title level="j">SIGGRAPH</title>
					<imprint>
						<date when="2018">2018</date>
					</imprint>
				</monogr>
			</biblStruct>

		</listBibl>
	</div>
		</back>
	</text>
</TEI>
