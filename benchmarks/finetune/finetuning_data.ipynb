{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c947eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3adbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON files...\n",
      "✓ Loaded train_single: 25,772 examples\n",
      "✓ Loaded valid_single: 1,235 examples\n",
      "✓ Loaded train_group: 725 examples\n",
      "✓ Loaded valid_group: 31 examples\n",
      "\n",
      "Total examples: 27,763\n"
     ]
    }
   ],
   "source": [
    "# Load data from JSON files\n",
    "data_dir = \"/Users/alex/docs/code/Odoma/citation_index/benchmarks/finetune\"\n",
    "\n",
    "print(\"Loading JSON files...\")\n",
    "with open(f\"{data_dir}/finetuning_train_single.json\", \"r\") as f:\n",
    "    train_single = json.load(f)\n",
    "print(f\"✓ Loaded train_single: {len(train_single):,} examples\")\n",
    "\n",
    "with open(f\"{data_dir}/finetuning_valid_single.json\", \"r\") as f:\n",
    "    valid_single = json.load(f)\n",
    "print(f\"✓ Loaded valid_single: {len(valid_single):,} examples\")\n",
    "\n",
    "with open(f\"{data_dir}/finetuning_train_group.json\", \"r\") as f:\n",
    "    train_group = json.load(f)\n",
    "print(f\"✓ Loaded train_group: {len(train_group):,} examples\")\n",
    "\n",
    "with open(f\"{data_dir}/finetuning_valid_group.json\", \"r\") as f:\n",
    "    valid_group = json.load(f)\n",
    "print(f\"✓ Loaded valid_group: {len(valid_group):,} examples\")\n",
    "\n",
    "print(f\"\\nTotal examples: {len(train_single) + len(valid_single) + len(train_group) + len(valid_group):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8508ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISTRIBUTION BY SOURCE\n",
      "============================================================\n",
      "\n",
      "train_single (25,772 examples):\n",
      "  cex         :    496 (  1.9%)\n",
      "  excite      :    661 (  2.6%)\n",
      "  linkedbook  : 24,615 ( 95.5%)\n",
      "\n",
      "valid_single (1,235 examples):\n",
      "  cex         :    173 ( 14.0%)\n",
      "  excite      :      7 (  0.6%)\n",
      "  linkedbook  :  1,055 ( 85.4%)\n",
      "\n",
      "train_group (725 examples):\n",
      "  cex         :     11 (  1.5%)\n",
      "  excite      :     33 (  4.6%)\n",
      "  linkedbook  :    681 ( 93.9%)\n",
      "\n",
      "valid_group (31 examples):\n",
      "  cex         :      5 ( 16.1%)\n",
      "  excite      :      2 (  6.5%)\n",
      "  linkedbook  :     24 ( 77.4%)\n"
     ]
    }
   ],
   "source": [
    "# Statistics: Distribution by source\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTRIBUTION BY SOURCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for split_name, split_data in [\n",
    "    (\"train_single\", train_single),\n",
    "    (\"valid_single\", valid_single),\n",
    "    (\"train_group\", train_group),\n",
    "    (\"valid_group\", valid_group)\n",
    "]:\n",
    "    source_counts = {}\n",
    "    for item in split_data:\n",
    "        source = item.get(\"source\", \"unknown\")\n",
    "        source_counts[source] = source_counts.get(source, 0) + 1\n",
    "    \n",
    "    print(f\"\\n{split_name} ({len(split_data):,} examples):\")\n",
    "    for source, count in sorted(source_counts.items()):\n",
    "        pct = (count / len(split_data)) * 100\n",
    "        print(f\"  {source:12s}: {count:6,} ({pct:5.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "230c406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LANGUAGE DISTRIBUTION (Single Dataset)\n",
      "============================================================\n",
      "\n",
      "train_single (25,772 examples):\n",
      "  IT          : 18,256 ( 70.8%)\n",
      "  EN          :  1,920 (  7.4%)\n",
      "  FR          :  1,712 (  6.6%)\n",
      "  DE          :  1,141 (  4.4%)\n",
      "  ES          :    801 (  3.1%)\n",
      "  en          :    606 (  2.4%)\n",
      "  de          :    551 (  2.1%)\n",
      "  PT          :    452 (  1.8%)\n",
      "  NL          :    301 (  1.2%)\n",
      "  UNKNOWN     :     32 (  0.1%)\n",
      "\n",
      "valid_single (1,235 examples):\n",
      "  IT          :    842 ( 68.2%)\n",
      "  en          :    173 ( 14.0%)\n",
      "  EN          :     88 (  7.1%)\n",
      "  DE          :     71 (  5.7%)\n",
      "  FR          :     46 (  3.7%)\n",
      "  de          :      7 (  0.6%)\n",
      "  ES          :      4 (  0.3%)\n",
      "  NL          :      2 (  0.2%)\n",
      "  PT          :      2 (  0.2%)\n"
     ]
    }
   ],
   "source": [
    "# Statistics: Language distribution (single dataset only)\n",
    "print(\"=\" * 60)\n",
    "print(\"LANGUAGE DISTRIBUTION (Single Dataset)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for split_name, split_data in [(\"train_single\", train_single), (\"valid_single\", valid_single)]:\n",
    "    lang_counts = {}\n",
    "    for item in split_data:\n",
    "        lang = item.get(\"language\", \"unknown\")\n",
    "        lang_counts[lang] = lang_counts.get(lang, 0) + 1\n",
    "    \n",
    "    print(f\"\\n{split_name} ({len(split_data):,} examples):\")\n",
    "    for lang, count in sorted(lang_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        pct = (count / len(split_data)) * 100\n",
    "        print(f\"  {lang:12s}: {count:6,} ({pct:5.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848cfe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REFERENCE COUNT DISTRIBUTION (Group Dataset)\n",
      "============================================================\n",
      "\n",
      "train_group (725 examples):\n",
      "  Min refs:    1\n",
      "  Max refs:    200\n",
      "  Mean refs:   35.7\n",
      "  Median refs: 30\n",
      "  Total refs:  25,893\n",
      "\n",
      "valid_group (31 examples):\n",
      "  Min refs:    1\n",
      "  Max refs:    156\n",
      "  Mean refs:   44.7\n",
      "  Median refs: 32\n",
      "  Total refs:  1,387\n"
     ]
    }
   ],
   "source": [
    "# Statistics: Reference count distribution (group dataset only)\n",
    "print(\"=\" * 60)\n",
    "print(\"REFERENCE COUNT DISTRIBUTION (Group Dataset)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for split_name, split_data in [(\"train_group\", train_group), (\"valid_group\", valid_group)]:\n",
    "    ref_counts = []\n",
    "    for item in split_data:\n",
    "        count = item.get(\"ref_count\", 0)\n",
    "        ref_counts.append(count)\n",
    "    \n",
    "    print(f\"\\n{split_name} ({len(split_data):,} examples):\")\n",
    "    print(f\"  Min refs:    {min(ref_counts):,}\")\n",
    "    print(f\"  Max refs:    {max(ref_counts):,}\")\n",
    "    print(f\"  Mean refs:   {np.mean(ref_counts):.1f}\")\n",
    "    print(f\"  Median refs: {np.median(ref_counts):.0f}\")\n",
    "    print(f\"  Total refs:  {sum(ref_counts):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e626c0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MESSAGE LENGTH STATISTICS (character counts)\n",
      "============================================================\n",
      "\n",
      "train_single (25,772 examples):\n",
      "  Min chars:    908\n",
      "  Max chars:    2,608\n",
      "  Mean chars:   1,357\n",
      "  Median chars: 1,360\n",
      "  Est. tokens:  339 (mean)\n",
      "\n",
      "valid_single (1,235 examples):\n",
      "  Min chars:    989\n",
      "  Max chars:    2,245\n",
      "  Mean chars:   1,425\n",
      "  Median chars: 1,428\n",
      "  Est. tokens:  356 (mean)\n",
      "\n",
      "train_group (725 examples):\n",
      "  Min chars:    1,005\n",
      "  Max chars:    155,319\n",
      "  Mean chars:   17,039\n",
      "  Median chars: 12,355\n",
      "  Est. tokens:  4260 (mean)\n",
      "\n",
      "valid_group (31 examples):\n",
      "  Min chars:    1,762\n",
      "  Max chars:    140,624\n",
      "  Mean chars:   31,334\n",
      "  Median chars: 19,037\n",
      "  Est. tokens:  7834 (mean)\n"
     ]
    }
   ],
   "source": [
    "# Statistics: Message length (approximate token count)\n",
    "print(\"=\" * 60)\n",
    "print(\"MESSAGE LENGTH STATISTICS (character counts)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for split_name, split_data in [\n",
    "    (\"train_single\", train_single),\n",
    "    (\"valid_single\", valid_single),\n",
    "    (\"train_group\", train_group),\n",
    "    (\"valid_group\", valid_group)\n",
    "]:\n",
    "    total_chars = []\n",
    "    for item in split_data:\n",
    "        total = sum(len(msg[\"content\"]) for msg in item[\"messages\"])\n",
    "        total_chars.append(total)\n",
    "    \n",
    "    print(f\"\\n{split_name} ({len(split_data):,} examples):\")\n",
    "    print(f\"  Min chars:    {min(total_chars):,}\")\n",
    "    print(f\"  Max chars:    {max(total_chars):,}\")\n",
    "    print(f\"  Mean chars:   {np.mean(total_chars):,.0f}\")\n",
    "    print(f\"  Median chars: {np.median(total_chars):,.0f}\")\n",
    "    print(f\"  Est. tokens:  {np.mean(total_chars)/4:.0f} (mean)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c49d85be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLE EXAMPLES\n",
      "============================================================\n",
      "\n",
      "--- SINGLE REFERENCE EXAMPLE ---\n",
      "Source: linkedbook\n",
      "Language: IT\n",
      "\n",
      "User message (first 200 chars):\n",
      "Goal: From the Input Text, detect all bibliographic references and return only this JSON object:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"references\": [\n",
      "    {\n",
      "      \"reference\": {\n",
      "        \"authors\": [\n",
      "          {\"first_name\": \"...\n",
      "\n",
      "Assistant response (first 300 chars):\n",
      "{\"references\": [{\"reference\": {\"authors\": [\"ROMANIN.\"], \"full_title\": \"Storia Documentata di Venezia, Venezia\", \"journal_title\": \"\", \"volume\": \"IH,\", \"issue\": \"\", \"pages\": \"pp. 300 - 301.\", \"publication_date\": \"1853,\", \"publisher\": \"\", \"publication_place\": \"\"}}]}...\n",
      "\n",
      "\n",
      "--- GROUP REFERENCE EXAMPLE ---\n",
      "Source: linkedbook\n",
      "Ref count: 25\n",
      "\n",
      "User message (first 300 chars):\n",
      "Convert these references to the following JSON structure:\n",
      "\n",
      "{\n",
      "    \"references\": [\n",
      "        {\n",
      "            \"reference\": {\n",
      "                \"authors\": [list of author objects | \"original string\"],\n",
      "                \"full_title\": \"string\",\n",
      "                \"journal_title\": \"string\",\n",
      "                \"volume\": ...\n",
      "\n",
      "Assistant response (first 400 chars):\n",
      "{\"references\": [{\"reference\": {\"authors\": [{\"first_name\": \"L.\", \"middle_name\": \"\", \"surname\": \"Gualdo Rosa\"}], \"full_title\": \"II Filelfo e i Tur - chi. Un inedito storico dell ’ Archivio Vaticano, « Annali della Facoltà di Lettere e Filo - sofia dell ’ Università di Napoli»,\", \"journal_title\": \"\", \"volume\": \"\", \"issue\": \"\", \"pages\": \"111 - 114,\", \"publication_date\": \"\", \"publisher\": \"\", \"publicati...\n"
     ]
    }
   ],
   "source": [
    "# Show sample examples\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Single example\n",
    "print(\"\\n--- SINGLE REFERENCE EXAMPLE ---\")\n",
    "sample_single = train_single[0]\n",
    "print(f\"Source: {sample_single.get('source')}\")\n",
    "print(f\"Language: {sample_single.get('language', 'N/A')}\")\n",
    "print(f\"\\nUser message (first 200 chars):\")\n",
    "print(sample_single['messages'][1]['content'][:200] + \"...\")\n",
    "print(f\"\\nAssistant response (first 300 chars):\")\n",
    "print(sample_single['messages'][2]['content'][:300] + \"...\")\n",
    "\n",
    "# Group example\n",
    "print(\"\\n\\n--- GROUP REFERENCE EXAMPLE ---\")\n",
    "sample_group = train_group[0]\n",
    "print(f\"Source: {sample_group.get('source')}\")\n",
    "print(f\"Ref count: {sample_group.get('ref_count')}\")\n",
    "print(f\"\\nUser message (first 300 chars):\")\n",
    "print(sample_group['messages'][1]['content'][:300] + \"...\")\n",
    "print(f\"\\nAssistant response (first 400 chars):\")\n",
    "print(sample_group['messages'][2]['content'][:400] + \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbce4f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALIZING DATASET FEATURES\n",
      "============================================================\n",
      "Normalizing single datasets...\n",
      "✓ All datasets normalized with consistent schema\n",
      "  Fields: messages, source, languages, ref_count\n"
     ]
    }
   ],
   "source": [
    "# Normalize features across all datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALIZING DATASET FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def normalize_single(item):\n",
    "    \"\"\"Add group-specific fields to single dataset items\"\"\"\n",
    "    normalized = item.copy()\n",
    "    # Single references always have ref_count = 1\n",
    "    normalized['ref_count'] = 1\n",
    "    # Convert single language string to list of languages\n",
    "    if 'language' in item:\n",
    "        normalized['languages'] = [item['language']]\n",
    "    else:\n",
    "        normalized['languages'] = []\n",
    "    \n",
    "    # remove language field\n",
    "    normalized.pop('language', None)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "print(\"Normalizing single datasets...\")\n",
    "train_single_normalized = [normalize_single(item) for item in train_single]\n",
    "valid_single_normalized = [normalize_single(item) for item in valid_single]\n",
    "\n",
    "print(\"✓ All datasets normalized with consistent schema\")\n",
    "print(\"  Fields: messages, source, languages, ref_count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ba1595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING HUGGINGFACE DATASET\n",
      "============================================================\n",
      "\n",
      "Dataset created successfully!\n",
      "DatasetDict({\n",
      "    train_single: Dataset({\n",
      "        features: ['messages', 'source', 'ref_count', 'languages'],\n",
      "        num_rows: 25772\n",
      "    })\n",
      "    valid_single: Dataset({\n",
      "        features: ['messages', 'source', 'ref_count', 'languages'],\n",
      "        num_rows: 1235\n",
      "    })\n",
      "    train_group: Dataset({\n",
      "        features: ['messages', 'source', 'ref_count', 'languages'],\n",
      "        num_rows: 725\n",
      "    })\n",
      "    valid_group: Dataset({\n",
      "        features: ['messages', 'source', 'ref_count', 'languages'],\n",
      "        num_rows: 31\n",
      "    })\n",
      "})\n",
      "\n",
      "✓ All splits have the same feature schema\n"
     ]
    }
   ],
   "source": [
    "# Create HuggingFace DatasetDict with normalized data\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING HUGGINGFACE DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to datasets with normalized data\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train_single\": Dataset.from_list(train_single_normalized),\n",
    "    \"valid_single\": Dataset.from_list(valid_single_normalized),\n",
    "    \"train_group\": Dataset.from_list(train_group),\n",
    "    \"valid_group\": Dataset.from_list(valid_group)\n",
    "})\n",
    "\n",
    "print(\"\\nDataset created successfully!\")\n",
    "print(dataset_dict)\n",
    "print(\"\\n✓ All splits have the same feature schema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b6ea855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PUSHING TO HUGGINGFACE HUB\n",
      "============================================================\n",
      "\n",
      "Pushing dataset to: reference-parsing-lora\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb97fe6d2534c6db6ce2b8d96cd5e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4b569be0f04483bc2e155a768f13aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c6652947db4dc48df123d19ba3c974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72389f69842f45ff916b492b744fe1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60952ae22ca14760b6c02e0bcdae59b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :   9%|8         |  582kB / 6.48MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd61e3d17b44d4788449544fb775604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d64bb27709e452c92cfa39ab8082e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546007dbc4514e6f9aa1625eecbfc2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90e778759be4d6c8b3aa101ca630f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c3822afe1e4c9989a5a214bb2d277e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        : 100%|##########|  272kB /  272kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9635f19cd44062bb4573fe9c9e4d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f85d4e25be4dbdba27eb213d92dfc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a4659a9a954fa290503162acbafb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b6f6adc128414f8b8f7eab347af4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9c33f9a9d746209afab289ac5f7f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  96%|#########6| 4.19MB / 4.36MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a86b450d7f742af879cbd89410c0548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5fb07b2a294570a925b91e3a5fbb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61628ea7398f4275be5adcbd067a4237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7df98644a0040ffac0b3caae6e12b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2ae5ca7ba149cabbc18575a5724cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        : 100%|##########|  397kB /  397kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60f245b848641c58de2aee2372e97ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset successfully pushed to HuggingFace Hub!\n",
      "View at: https://huggingface.co/datasets/reference-parsing-lora\n"
     ]
    }
   ],
   "source": [
    "# Push to HuggingFace Hub\n",
    "print(\"=\" * 60)\n",
    "print(\"PUSHING TO HUGGINGFACE HUB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Repository name\n",
    "repo_name = \"reference-parsing-lora\"\n",
    "\n",
    "# Push to hub with description\n",
    "print(f\"\\nPushing dataset to: {repo_name}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    repo_name,\n",
    "    private=False,  # Set to True if you want a private dataset\n",
    "    commit_message=\"Upload with normalized schema across all splits\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataset successfully pushed to HuggingFace Hub!\")\n",
    "print(f\"View at: https://huggingface.co/datasets/{repo_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "710c7c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING NO-PDF VERSION, MERGING, AND PUSHING\n",
      "============================================================\n",
      " Loaded train_group_nopdf: 725 examples\n",
      "  Loaded valid_group_nopdf: 31 examples\n",
      " train: 26,497 examples (single + group_nopdf)\n",
      "  valid: 1,266 examples (single + group_nopdf)\n",
      " 27,763 total examples\n",
      "   Repository: yurui983/reference-parsing-lora-mix\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7315ff3e41e84251aebb603d0dc7bc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1b18b50bc04b7ba92e1c48353d794a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267dcdbd0c75467985530fcd5c0421d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5c7bdb8aea4755b1144b576ab90b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c3379e962f413ba732627f0f9bc5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  97%|#########7| 9.43MB / 9.68MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb553a9dedf4ec389747022f3292cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f979f155cab4d8ab2679afba9e82389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3eef3eab5e34c9ab02c24ed8598a59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e348c76d1e4ebc82ced5e213eda212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8c55f764d94f109433f6244b782f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  46%|####6     |  209kB /  451kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ea466069664d9f87af20adb19ecc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/562 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully pushed to HuggingFace Hub!\n",
      "   View at: https://huggingface.co/datasets/yurui983/reference-parsing-lora-mix\n"
     ]
    }
   ],
   "source": [
    "# Load no-PDF group datasets, merge with single, and push to HuggingFace Hub\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING NO-PDF VERSION, MERGING, AND PUSHING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load no-PDF group datasets\n",
    "with open(f\"{data_dir}/finetuning_train_group_nopdf.json\", \"r\") as f:\n",
    "    train_group_nopdf = json.load(f)\n",
    "print(f\" Loaded train_group_nopdf: {len(train_group_nopdf):,} examples\")\n",
    "\n",
    "with open(f\"{data_dir}/finetuning_valid_group_nopdf.json\", \"r\") as f:\n",
    "    valid_group_nopdf = json.load(f)\n",
    "print(f\"  Loaded valid_group_nopdf: {len(valid_group_nopdf):,} examples\")\n",
    "\n",
    "# Merge with single normalized datasets\n",
    "\n",
    "train_mix_nopdf = train_single_normalized.copy()\n",
    "train_mix_nopdf.extend(train_group_nopdf)\n",
    "print(f\" train: {len(train_mix_nopdf):,} examples (single + group_nopdf)\")\n",
    "\n",
    "valid_mix_nopdf = valid_single_normalized.copy()\n",
    "valid_mix_nopdf.extend(valid_group_nopdf)\n",
    "print(f\"  valid: {len(valid_mix_nopdf):,} examples (single + group_nopdf)\")\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "dataset_dict_nopdf_mix = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_mix_nopdf),\n",
    "    \"valid\": Dataset.from_list(valid_mix_nopdf)\n",
    "})\n",
    "print(f\" {len(train_mix_nopdf) + len(valid_mix_nopdf):,} total examples\")\n",
    "\n",
    "# Push to HuggingFace Hub\n",
    "\n",
    "repo_name = \"yurui983/reference-parsing-lora-mix\"\n",
    "print(f\"   Repository: {repo_name}\")\n",
    "\n",
    "dataset_dict_nopdf_mix.push_to_hub(\n",
    "    repo_name,\n",
    "    private=False,\n",
    "    commit_message=\"Update with no-PDF version: single + group_nopdf (references only, no full PDF text)\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Successfully pushed to HuggingFace Hub!\")\n",
    "print(f\"   View at: https://huggingface.co/datasets/{repo_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c7fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citation_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
