{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citation_index.core.extractors import ExtractorFactory\n",
    "import pandas as pd\n",
    "import json\n",
    "from Levenshtein import ratio as levenshtein_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total references: 10171\n"
     ]
    }
   ],
   "source": [
    "def get_sample_data(pdf_df, references_data, n_samples=5):\n",
    "    \"\"\"Display sample data for verification\"\"\"\n",
    "    print(f\"\\n=== SAMPLE PDF DATA ===\")\n",
    "    if not pdf_df.empty:\n",
    "        print(pdf_df.sample(n_samples).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n=== SAMPLE REFERENCES DATA ===\")\n",
    "    if references_data:\n",
    "        sample_keys = list(references_data.keys())[:n_samples]\n",
    "        for i, file_id in enumerate(sample_keys):\n",
    "            ref_data = references_data[file_id]\n",
    "            print(f\"Paper {i+1} (ID: {file_id}):\")\n",
    "            print(f\"  Number of references: {len(ref_data['references'])}\")\n",
    "            print(f\"  First few references:\")\n",
    "            for j, ref in enumerate(ref_data['references'][:3]):\n",
    "                print(f\"    {j+1}. {ref}\")\n",
    "            if len(ref_data['references']) > 3:\n",
    "                print(f\"    ... and {len(ref_data['references']) - 3} more\")\n",
    "            print()\n",
    "\n",
    "\n",
    "pdf_df = pd.read_csv(\"../../EXgoldstandard/Goldstandard_EXparser/pdf_files_info.csv\")\n",
    "references_data = json.load(open(\"../../EXgoldstandard/Goldstandard_EXparser/all_references.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "total_references = sum(len(data[\"references\"]) for data in references_data.values())\n",
    "print('Total references:', total_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citation_index.llm.prompt_loader import ReferenceExtractionAndParsingPrompt\n",
    "from citation_index.llm.client import LLMClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m api_key \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msk-282f6b9a54b64bd98bfcd85c0c8f5aab\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# deepseek\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# endpoint = 'https://api.anthropic.com/v1/'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# model = 'claude-sonnet-4-20250514'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# api_key = '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'\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m client \u001b[39m=\u001b[39m LLMClient(endpoint, model, api_key)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../../EXgoldstandard/Goldstandard_EXparser/all_pdfs/1181.pdf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m extractor \u001b[39m=\u001b[39m ExtractorFactory\u001b[39m.\u001b[39mcreate(\u001b[39m\"\u001b[39m\u001b[39mpymupdf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LLMClient' is not defined"
     ]
    }
   ],
   "source": [
    "endpoint = 'https://api.deepseek.com/v1'\n",
    "model = 'deepseek-chat'\n",
    "api_key = 'sk-282f6b9a54b64bd98bfcd85c0c8f5aab' # deepseek\n",
    "\n",
    "# endpoint = 'https://api.anthropic.com/v1/'\n",
    "# model = 'claude-sonnet-4-20250514'\n",
    "# api_key = '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'\n",
    "\n",
    "client = LLMClient(endpoint, model, api_key)\n",
    "\n",
    "filepath = '../../EXgoldstandard/Goldstandard_EXparser/all_pdfs/1181.pdf'\n",
    "\n",
    "extractor = ExtractorFactory.create(\"pymupdf\")\n",
    "result = extractor.extract(filepath)\n",
    "\n",
    "prompts = ReferenceExtractionAndParsingPrompt(input_text=result, prompt = '/Users/alex/docs/code/Odoma/citation_index/prompts/reference_extraction_and_parsing.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(response: str) -> dict:\n",
    "    # remove markdown\n",
    "    if response.startswith(\"```\"):\n",
    "        response = \"\\n\".join([line for line in response.split(\"\\n\")][1:-1])\n",
    "        # print(response)\n",
    "\n",
    "    try:\n",
    "        # Parse the JSON response\n",
    "        data = json.loads(response)\n",
    "        \n",
    "        # Validate the structure matches our expected schema\n",
    "        if not isinstance(data, dict) or \"references\" not in data:\n",
    "            print(\"Invalid response format: missing 'references' key\")\n",
    "            return {\"references\": []}\n",
    "            \n",
    "        references = data[\"references\"]\n",
    "        if not isinstance(references, list):\n",
    "            print(\"Invalid response format: 'references' is not a list\")\n",
    "            return {\"references\": []}\n",
    "            \n",
    "        # Validate each reference has the expected structure\n",
    "        valid_references = []\n",
    "        for ref in references:\n",
    "            if not isinstance(ref, dict) or \"reference\" not in ref:\n",
    "                continue\n",
    "                \n",
    "            ref_data = ref[\"reference\"]\n",
    "            if not isinstance(ref_data, dict):\n",
    "                continue\n",
    "                \n",
    "            # Check for required fields\n",
    "            if not all(key in ref_data for key in [\"authors\", \"title\"]):\n",
    "                continue\n",
    "                \n",
    "            # Validate authors\n",
    "            authors = ref_data.get(\"authors\", [])\n",
    "            if not isinstance(authors, list):\n",
    "                continue\n",
    "                \n",
    "            valid_authors = []\n",
    "            for author in authors:\n",
    "                if not isinstance(author, dict):\n",
    "                    continue\n",
    "                if not all(key in author for key in [\"first_name\", \"surname\"]):\n",
    "                    continue\n",
    "                valid_authors.append(author)\n",
    "            \n",
    "            if valid_authors:\n",
    "                ref_data[\"authors\"] = valid_authors\n",
    "                valid_references.append({\"reference\": ref_data})\n",
    "        \n",
    "        return {\"references\": valid_references}\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError: {e}\")\n",
    "        return {\"references\": []}\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        return {\"references\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.call(prompts.prompt)\n",
    "# parse(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m references_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# matrixs = []\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pdf_df\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m pdf_df[\u001b[39m'\u001b[39m\u001b[39mfile_id\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m prompts_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/Users/alex/docs/code/Odoma/citation_index/prompts/reference_extraction_and_parsing.md\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/docs/code/Odoma/citation_index/benchmarks/benchmarking/citation_extraction_and_parsing_eval.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m tqdm(pdf_df[\u001b[39m'\u001b[39m\u001b[39mfile_id\u001b[39m\u001b[39m'\u001b[39m]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf_df' is not defined"
     ]
    }
   ],
   "source": [
    "# run on whole excite dataset\n",
    "extractor = ExtractorFactory.create(\"pymupdf\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "response_list = []\n",
    "references_list = []\n",
    "# matrixs = []\n",
    "pdf_df.index = pdf_df['file_id']\n",
    "prompts_dir = '/Users/alex/docs/code/Odoma/citation_index/prompts/reference_extraction_and_parsing.md'\n",
    "\n",
    "\n",
    "for id in tqdm(pdf_df['file_id']):\n",
    "    try:\n",
    "        if pdf_df.loc[id, 'page_count'] > 100:\n",
    "            print(f\"Skipping {id} because page_count is too large\")\n",
    "            continue\n",
    "        filepath = f'../../EXgoldstandard/Goldstandard_EXparser/all_pdfs/{id}.pdf'\n",
    "        result = extractor.extract(filepath)\n",
    "        \n",
    "        # Split text into chunks if needed\n",
    "        prompts = ReferenceExtractionAndParsingPrompt(input_text=result, prompt = prompts_dir)\n",
    "        \n",
    "        response = client.call(prompts.prompt)\n",
    "\n",
    "        references = parse(response)\n",
    "\n",
    "        response_list.append({'id': id, 'response': response})\n",
    "        references_list.append({'id': id, 'references': references})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Save response_list with error handling\n",
    "try:\n",
    "    with open('response_ref_extparsing_deepseek_pymupdf.json', 'w') as f:\n",
    "        json.dump(response_list, f)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving response list: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load grouth truth from xml\n",
    "from lxml import etree\n",
    "from typing import Optional, List\n",
    "from pathlib import Path\n",
    "from citation_index.core.models import References, Reference\n",
    "\n",
    "xml_dir = '../../EXgoldstandard/Goldstandard_EXparser/all_xml'\n",
    "\n",
    "# read xml files\n",
    "from citation_index.core.models import References\n",
    "\n",
    "filepath = '../../EXgoldstandard/Goldstandard_EXparser/all_xml/1181.xml'\n",
    "references = References.from_excite_xml(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(references, references_gt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run on whole excite dataset\n",
    "# extractor = ExtractorFactory.create(\"pymupdf\")\n",
    "# from tqdm import tqdm\n",
    "# import concurrent.futures\n",
    "# import json\n",
    "# import threading\n",
    "\n",
    "# # Create a lock for PyMuPDF operations\n",
    "# pymupdf_lock = threading.Lock()\n",
    "\n",
    "# def process_file(id):\n",
    "#     try:\n",
    "#         if pdf_df.loc[id, 'page_count'] > 100:\n",
    "#             print(f\"Skipping {id} because page_count is too large\")\n",
    "#             return None, None\n",
    "            \n",
    "#         filepath = f'../../EXgoldstandard/Goldstandard_EXparser/all_pdfs/{id}.pdf'\n",
    "        \n",
    "#         # Use lock when extracting with PyMuPDF\n",
    "#         with pymupdf_lock:\n",
    "#             result = extractor.extract(filepath)\n",
    "        \n",
    "#         # Split text into chunks if needed\n",
    "#         prompts = ReferenceExtractionAndParsingPrompt(input_text=result, prompt=prompts_dir)\n",
    "        \n",
    "#         response = client.call(prompts.prompt)\n",
    "#         references = parse(response)\n",
    "        \n",
    "#         return {'id': id, 'response': response}, {'id': id, 'references': references}\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing file {id}: {str(e)}\")\n",
    "#         return None, None\n",
    "\n",
    "# response_list = []\n",
    "# references_list = []\n",
    "# pdf_df.index = pdf_df['file_id']\n",
    "# prompts_dir = '/Users/alex/docs/code/Odoma/citation_index/prompts/reference_extraction_and_parsing.md'\n",
    "\n",
    "# # Process files concurrently with max 10 workers\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "#     # Submit all tasks\n",
    "#     future_to_id = {executor.submit(process_file, id): id for id in pdf_df['file_id']}\n",
    "    \n",
    "#     # Process results as they complete\n",
    "#     for future in tqdm(concurrent.futures.as_completed(future_to_id), total=len(future_to_id)):\n",
    "#         response_data, references_data = future.result()\n",
    "#         if response_data is not None:\n",
    "#             response_list.append(response_data)\n",
    "#         if references_data is not None:\n",
    "#             references_list.append(references_data)\n",
    "\n",
    "# # Save response_list with error handling\n",
    "# try:\n",
    "#     with open('response_ref_extparsing_deepseek_pymupdf.json', 'w') as f:\n",
    "#         json.dump(response_list, f)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error saving response list: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, Field, ValidationError, create_model\n",
    "# from citation_index.core.models import References, Reference\n",
    "# def parse(response: str) -> References:\n",
    "#         # remove markdown\n",
    "#         if response.startswith(\"```\"):\n",
    "#             response = \"\\n\".join([line for line in response.split(\"\\n\")][1:-1])\n",
    "\n",
    "#         try:\n",
    "#             references = References.model_validate_json(response).references\n",
    "#         except ValidationError as e:\n",
    "#             print(f\"ValidationError: {e}\")\n",
    "#             # _LOGGER.debug(f\"ValidationError: {e}\")\n",
    "#             references = []\n",
    "\n",
    "#         references = [ref for ref in references if ref != Reference()]\n",
    "\n",
    "#         return References(references=references)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
