{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citation_index.core.extractors import ExtractorFactory\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import random\n",
    "from Levenshtein import ratio as levenshtein_ratio\n",
    "\n",
    "from citation_index.llm.prompt_loader import ReferenceExtractionAndParsingPrompt\n",
    "from citation_index.llm.client import LLMClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total references: 10171\n"
     ]
    }
   ],
   "source": [
    "def get_sample_data(pdf_df, references_data, n_samples=5):\n",
    "    \"\"\"Display sample data for verification\"\"\"\n",
    "    print(f\"\\n=== SAMPLE PDF DATA ===\")\n",
    "    if not pdf_df.empty:\n",
    "        print(pdf_df.sample(n_samples).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n=== SAMPLE REFERENCES DATA ===\")\n",
    "    if references_data:\n",
    "        sample_keys = list(references_data.keys())[:n_samples]\n",
    "        for i, file_id in enumerate(sample_keys):\n",
    "            ref_data = references_data[file_id]\n",
    "            print(f\"Paper {i+1} (ID: {file_id}):\")\n",
    "            print(f\"  Number of references: {len(ref_data['references'])}\")\n",
    "            print(f\"  First few references:\")\n",
    "            for j, ref in enumerate(ref_data['references'][:3]):\n",
    "                print(f\"    {j+1}. {ref}\")\n",
    "            if len(ref_data['references']) > 3:\n",
    "                print(f\"    ... and {len(ref_data['references']) - 3} more\")\n",
    "            print()\n",
    "\n",
    "\n",
    "pdf_df = pd.read_csv(\"../../EXgoldstandard/Goldstandard_EXparser/pdf_files_info.csv\")\n",
    "references_data = json.load(open(\"../../EXgoldstandard/Goldstandard_EXparser/all_references.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "total_references = sum(len(data[\"references\"]) for data in references_data.values())\n",
    "print('Total references:', total_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# endpoint = 'https://api.anthropic.com/v1/'\n",
    "# model = 'claude-sonnet-4-20250514'\n",
    "# api_key = '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'\n",
    "\n",
    "# client = LLMClient(endpoint, model, api_key)\n",
    "\n",
    "filepath = '../../EXgoldstandard/Goldstandard_EXparser/all_pdfs/1181.pdf'\n",
    "\n",
    "extractor = ExtractorFactory.create(\"pymupdf\")\n",
    "result = extractor.extract(filepath)\n",
    "\n",
    "prompts = ReferenceExtractionAndParsingPrompt(input_text=result, prompt = '/Users/alex/docs/code/Odoma/citation_index/prompts/reference_extraction_and_parsing.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(response: str,start_tag: str = \"<start>\",end_tag: str = \"<end>\") -> dict:\n",
    "    # remove markdown\n",
    "    if response.startswith(start_tag):\n",
    "        response = response[len(start_tag):]\n",
    "    if response.endswith(end_tag):\n",
    "        response = response[:-len(end_tag)]\n",
    "    if response.startswith(\"```\"):\n",
    "        response = \"\\n\".join([line for line in response.split(\"\\n\")][1:-1])\n",
    "        # print(response)\n",
    "\n",
    "    \n",
    "        # Parse the JSON response\n",
    "    data = json.loads(response)\n",
    "        \n",
    "        # Validate the structure matches our expected schema\n",
    "    if not isinstance(data, dict) or \"references\" not in data:\n",
    "        print(\"Invalid response format: missing 'references' key\")\n",
    "        return {\"references\": []}\n",
    "            \n",
    "    references = data[\"references\"]\n",
    "    if not isinstance(references, list):\n",
    "        print(\"Invalid response format: 'references' is not a list\")\n",
    "        return {\"references\": []}\n",
    "            \n",
    "    # Validate each reference has the expected structure\n",
    "    valid_references = []\n",
    "    for ref in references:\n",
    "        if not isinstance(ref, dict) or \"reference\" not in ref:\n",
    "            continue\n",
    "                \n",
    "        ref_data = ref[\"reference\"]\n",
    "        if not isinstance(ref_data, dict):\n",
    "            continue\n",
    "                \n",
    "        # Check for required fields\n",
    "        if not all(key in ref_data for key in [\"authors\", \"title\"]):\n",
    "            continue\n",
    "                \n",
    "        # Validate authors\n",
    "        authors = ref_data.get(\"authors\", [])\n",
    "        if not isinstance(authors, list):\n",
    "            continue\n",
    "                \n",
    "        valid_authors = []\n",
    "        for author in authors:\n",
    "            if not isinstance(author, dict):\n",
    "                continue\n",
    "        if not all(key in author for key in [\"first_name\", \"surname\"]):\n",
    "                continue\n",
    "        valid_authors.append(author)\n",
    "            \n",
    "        if valid_authors:\n",
    "            ref_data[\"authors\"] = valid_authors\n",
    "            valid_references.append({\"reference\": ref_data})\n",
    "        \n",
    "    return {\"references\": valid_references}\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_api_with_backoff(client, prompt, start_tag, end_tag, max_tokens, max_retries=3):\n",
    "    \"\"\"Call API with exponential backoff for rate limiting\"\"\"\n",
    "    base_delay = 60  # Start with 60 seconds\n",
    "    max_delay = 180  # Max delay of 5 minutes\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            messages, response = client.call_with_continuation(\n",
    "                prompt=prompt,\n",
    "                start_tag=start_tag,\n",
    "                end_tag=end_tag,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return messages, response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"rate limit\" in error_msg or \"429\" in error_msg:\n",
    "                if attempt < max_retries - 1:\n",
    "                    # Exponential backoff with jitter\n",
    "                    delay = min(base_delay * (2 ** attempt) + random.uniform(0, 10), max_delay)\n",
    "                    print(f\"Rate limit hit, waiting {delay:.1f} seconds before retry {attempt + 1}/{max_retries}\")\n",
    "                    sleep(delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Max retries reached for rate limiting\")\n",
    "                    raise e\n",
    "            else:\n",
    "                # Non-rate-limit error, don't retry\n",
    "                raise e\n",
    "    \n",
    "    raise Exception(\"Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on whole excite dataset\n",
    "extractor = ExtractorFactory.create(\"pymupdf\")\n",
    "\n",
    "\n",
    "# endpoint = 'https://api.deepseek.com/v1'\n",
    "# model = 'deepseek-chat'\n",
    "# api_key = 'sk-282f6b9a54b64bd98bfcd85c0c8f5aab' # deepseek\n",
    "# MAX_OUTPUT_TOKEN = 8192\n",
    "\n",
    "# endpoint = 'https://api.anthropic.com/v1/'\n",
    "# model = 'claude-sonnet-4-20250514'\n",
    "# api_key = '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'\n",
    "# MAX_OUTPUT_TOKEN = 20000\n",
    "\n",
    "# gemma put max 50000 context window\n",
    "endpoint = 'https://cy2uoaiag9ska4-8000.proxy.runpod.net/v1'\n",
    "model  = 'google/gemma-3-27b-it'\n",
    "api_key = 'rpa_BTOUM8PPM4I9XZDM1ASDRRH90K8GVYWTXRQ70IYN3qc0q0'\n",
    "MAX_OUTPUT_TOKEN = 3000\n",
    "client = LLMClient(endpoint, model, api_key)\n",
    "\n",
    "\n",
    "response_list = []\n",
    "references_list = []\n",
    "# matrixs = []\n",
    "pdf_df.index = pdf_df['file_id']\n",
    "prompts_path = '/Users/alex/docs/code/Odoma/citation_index/prompts/reference_extraction_and_parsing.md'\n",
    "\n",
    "pbar = tqdm(pdf_df['file_id'], desc=\"ðŸ“„ Processing PDFs\", unit=\"file\", \n",
    "           bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n",
    "\n",
    "\n",
    "\n",
    "for id in pbar:\n",
    "    try:\n",
    "        if pdf_df.loc[id, 'page_count'] > 100:\n",
    "            print(f\"Skipping {id} because page_count is too large\")\n",
    "            continue\n",
    "            \n",
    "        filepath = f'../../EXgoldstandard/Goldstandard_EXparser/all_pdfs/{id}.pdf'\n",
    "        result = extractor.extract(filepath)\n",
    "        \n",
    "        # Split text into chunks if needed\n",
    "        prompts = ReferenceExtractionAndParsingPrompt(input_text=result, prompt=prompts_path)\n",
    "        \n",
    "        # Use the API call with backoff\n",
    "        try:\n",
    "            messages, response = client.call_with_continuation(\n",
    "                prompt=prompts.prompt,\n",
    "                start_tag='<start>',\n",
    "                end_tag=['<end>','</end>'],\n",
    "                max_tokens=MAX_OUTPUT_TOKEN\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"API call failed for file {id}: {str(e)}\")\n",
    "            response_list.append({'id': id, 'response': None})\n",
    "            references_list.append({'id': id, 'references': {\"references\": []}})\n",
    "            continue\n",
    "        \n",
    "        # Parse the response with retry logic for JSON formatting errors\n",
    "        retry_count = 0\n",
    "        max_retries = 2\n",
    "        references = None\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                references = parse(response)\n",
    "                break  # Success, exit retry loop\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSONDecodeError for file {id} (attempt {retry_count + 1}): {e}\")\n",
    "                retry_count += 1\n",
    "                if retry_count >= max_retries:\n",
    "                    print(f\"Failed to parse JSON for file {id} after {max_retries} attempts\")\n",
    "                    references = {\"references\": []}  \n",
    "                    break \n",
    "                # Retry API call for JSON parsing errors\n",
    "                try:\n",
    "                    messages, response = client.call_with_continuation(\n",
    "                        prompt=prompts.prompt,\n",
    "                        start_tag='<start>',\n",
    "                        end_tag=['<end>','</end>'],\n",
    "                        max_tokens=MAX_OUTPUT_TOKEN\n",
    "                    )\n",
    "                except Exception as api_e:\n",
    "                    print(f\"API retry failed for JSON parsing: {str(api_e)}\")\n",
    "                    references = {\"references\": []}\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error parsing file {id}: {str(e)}\")\n",
    "                references = {\"references\": []}  # Default empty result\n",
    "                break\n",
    "\n",
    "        response_list.append({'id': id, 'response': response})\n",
    "        references_list.append({'id': id, 'references': references})\n",
    "        \n",
    "        # Add a small delay between successful requests to be extra safe\n",
    "        # sleep(random.uniform(5, 15))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {id}: {str(e)}\")\n",
    "        response_list.append({'id': id, 'response': None})\n",
    "        references_list.append({'id': id, 'references': {\"references\": []}})\n",
    "        continue\n",
    "\n",
    "# Save response_list with error handling\n",
    "try:\n",
    "    with open('response_ref_extparsing_gemma_pymupdf.json', 'w') as f:\n",
    "        json.dump(response_list, f)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving response list: {str(e)}\")\n",
    "\n",
    "# save parsed references in pickle\n",
    "import pickle\n",
    "with open('references_ref_extparsing_gemma_pymupdf.pkl', 'wb') as f:\n",
    "    pickle.dump(references_list, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citation_index.core.extractors import ExtractorFactory\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from citation_index.core.models import References\n",
    "from citation_index.evaluation.ref_metrics import RefEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prediction for 32965\n",
      "No prediction for 28444\n",
      "No prediction for 12526\n",
      "No prediction for 27667\n",
      "No prediction for 26236\n",
      "No prediction for 22654\n",
      "No prediction for 4930\n",
      "No prediction for 6026 \n",
      "No prediction for 35267\n",
      "No prediction for 18268 \n",
      "No prediction for 44849\n",
      "No prediction for 32707\n",
      "No prediction for 42768\n",
      "No prediction for 48208\n",
      "No prediction for 42309\n",
      "Total files: 350\n",
      "Missing predictions: 15\n",
      "Missing GTs: 0\n",
      "Reference eval (exact): {'precision': 0.7046529968454258, 'recall': 0.5087091895738813, 'micro_f1': 0.5908597761957408, 'macro_f1': 0.5243078285633297, 'per_class_f1': {'monographic_title': 0.6608767576509513, 'authors': 0.6010647432594883, 'publisher': 0.467693193265218, 'publication_date': 0.8245757900712798, 'publication_place': 0.0, 'analytic_title': 0.6730746688547649, 'journal_title': 0.6605086098756985, 'volume': 0.7485127095727419, 'pages': 0.5799196787148594, 'editors': 0.0, 'issue': 0.7449127906976745, 'refs': 0.0, 'footnote_number': 0.0, 'translator': 0.0, 'cited_range': 0.0}}\n",
      "   precision    recall  micro_f1  macro_f1 file_id\n",
      "0   0.831050  0.686792  0.752066  0.766125   36325\n",
      "1   0.900433  0.707483  0.792381  0.792302    9082\n",
      "2   0.572864  0.456000  0.507795  0.495594   38687\n",
      "3   0.815094  0.717608  0.763251  0.735187   20786\n",
      "4   0.502994  0.254545  0.338028  0.336129   18437\n"
     ]
    }
   ],
   "source": [
    "evaluator = RefEvaluator(mode='exact')\n",
    "from excite_helper import evaluate_whole_dataset\n",
    "\n",
    "\n",
    "pred_pkl_path = \"references_ref_extparsing_deepseek_pymupdf.pkl\"\n",
    "xml_dir = \"../../EXgoldstandard/Goldstandard_EXparser/all_xml\"\n",
    "overall_metrics, per_doc_df = evaluate_whole_dataset(pred_pkl_path, xml_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prediction for 32965\n",
      "No prediction for 28444\n",
      "No prediction for 12526\n",
      "No prediction for 27667\n",
      "No prediction for 26236\n",
      "No prediction for 22654\n",
      "No prediction for 4930\n",
      "No prediction for 6026 \n",
      "No prediction for 35267\n",
      "No prediction for 18268 \n",
      "No prediction for 44849\n",
      "No prediction for 32707\n",
      "No prediction for 42768\n",
      "No prediction for 48208\n",
      "No prediction for 42309\n",
      "Total files: 350\n",
      "Missing predictions: 15\n",
      "Missing GTs: 0\n",
      "Reference eval (exact): {'precision': 0.8787384448069603, 'recall': 0.7909936368086148, 'micro_f1': 0.8325605358062854, 'macro_f1': 0.7000391143233026, 'per_class_f1': {'authors': 0.8325605358062854}}\n",
      "   precision    recall  micro_f1  macro_f1 file_id\n",
      "0   0.822581  0.980769  0.894737  0.909091   36325\n",
      "1   0.886792  0.959184  0.921569  0.918182    9082\n",
      "2   1.000000  0.957447  0.978261  0.963964   38687\n",
      "3   0.924528  0.907407  0.915888  0.816667   20786\n",
      "4   0.869565  0.769231  0.816327  0.711770   18437\n"
     ]
    }
   ],
   "source": [
    "overall_metrics, per_doc_df = evaluate_whole_dataset(pred_pkl_path, xml_dir, \n",
    "                                                     focus_fields=['authors','title','year','doi'], \n",
    "                                                     fuzzy_threshold=90,\n",
    "                                                     mode='fuzzy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>lang</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>macro_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>0.766069</td>\n",
       "      <td>0.825987</td>\n",
       "      <td>0.778546</td>\n",
       "      <td>0.692326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>0.875189</td>\n",
       "      <td>0.888740</td>\n",
       "      <td>0.870518</td>\n",
       "      <td>0.821393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>de</td>\n",
       "      <td>0.625801</td>\n",
       "      <td>0.687841</td>\n",
       "      <td>0.633569</td>\n",
       "      <td>0.370871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>de</td>\n",
       "      <td>0.791721</td>\n",
       "      <td>0.567967</td>\n",
       "      <td>0.647094</td>\n",
       "      <td>0.435181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class lang  precision    recall  micro_f1  macro_f1\n",
       "0      1   de   0.766069  0.825987  0.778546  0.692326\n",
       "1      1   en   0.875189  0.888740  0.870518  0.821393\n",
       "2      2   de   0.625801  0.687841  0.633569  0.370871\n",
       "3      3   de   0.791721  0.567967  0.647094  0.435181"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_doc_df\n",
    "\n",
    "pdf_df = pd.read_csv(\"../../EXgoldstandard/Goldstandard_EXparser/pdf_files_info.csv\")\n",
    "pdf_df = pdf_df.reset_index(drop=True)\n",
    "pdf_df['file_id'] = pdf_df['file_id'].astype(str)\n",
    "pdf_df_deepseek = pd.merge(pdf_df, per_doc_df, on='file_id', how='left')\n",
    "pdf_df_deepseek.head()\n",
    "\n",
    "# group by class and lang and calculate avg precision, recall, f1_score, avg_levenshtein_ratio\n",
    "pdf_df_deepseek.groupby(['class', 'lang']).agg({'precision': 'mean', 'recall': 'mean', 'micro_f1': 'mean', 'macro_f1': 'mean'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
