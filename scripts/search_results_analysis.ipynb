{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a046cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ac9f2",
   "metadata": {},
   "source": [
    "# Citation Search Result Coverage and HF Dataset Prep\n",
    "\n",
    "This notebook loads the aggregated search results, reports coverage statistics for each external source, and prepares a Hugging Face DatasetDict with one split per reference source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e7c54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 759 reference records from results_20251020_124218_limitNone_openalex_wikidata_matilda.json\n",
      "Loaded 759 reference records from results_20251022_161614_limitNone_opencitations.json\n",
      "Reference sources (main): ['cex', 'excite', 'linkedbook']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DATA_PATH = Path(\"results_20251020_124218_limitNone_openalex_wikidata_matilda.json\")\n",
    "OPEN_CITATION_DATA_PATH = Path(\"results_20251022_161614_limitNone_opencitations.json\")\n",
    "\n",
    "with DATA_PATH.open() as f:\n",
    "    records = json.load(f)\n",
    "print(f\"Loaded {len(records)} reference records from {DATA_PATH}\")\n",
    "\n",
    "# Load OpenCitations records (separate file)\n",
    "with OPEN_CITATION_DATA_PATH.open() as f:\n",
    "    records_opencitations = json.load(f)\n",
    "print(f\"Loaded {len(records_opencitations)} reference records from {OPEN_CITATION_DATA_PATH}\")\n",
    "\n",
    "sources = sorted({record.get(\"source\", \"unknown\") for record in records})\n",
    "print(f\"Reference sources (main): {sources}\")\n",
    "providers_main = [\"openalex\", \"matilda\", \"wikidata\"]\n",
    "\n",
    "# fix the omid mapping for opencitations\n",
    "for record in records_opencitations:\n",
    "    ids = record.get(\"ids\", {})\n",
    "    omid = ids.get(\"omid\")\n",
    "    if omid:\n",
    "        cleaned = omid.rstrip(\"/\")\n",
    "        parts = cleaned.split(\"/\")\n",
    "        # Return last two segments, e.g., \"br/06210459208\"\n",
    "        if len(parts) >= 2:\n",
    "            ids[\"omid\"] = \"/\".join(parts[-2:])\n",
    "    record[\"ids\"] = ids\n",
    "providers_opencitations = [\"opencitations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f436d0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success_count</th>\n",
       "      <th>success_rate</th>\n",
       "      <th>top_result_count</th>\n",
       "      <th>top_result_rate</th>\n",
       "      <th>is_match_count</th>\n",
       "      <th>is_match_rate</th>\n",
       "      <th>year_match_count</th>\n",
       "      <th>year_match_rate</th>\n",
       "      <th>mean_title_similarity</th>\n",
       "      <th>mean_author_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>openalex</th>\n",
       "      <td>757.0</td>\n",
       "      <td>0.997365</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.781291</td>\n",
       "      <td>383.0</td>\n",
       "      <td>0.504611</td>\n",
       "      <td>385.0</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>87.416526</td>\n",
       "      <td>61.976391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matilda</th>\n",
       "      <td>757.0</td>\n",
       "      <td>0.997365</td>\n",
       "      <td>421.0</td>\n",
       "      <td>0.554677</td>\n",
       "      <td>314.0</td>\n",
       "      <td>0.413702</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.368906</td>\n",
       "      <td>99.337292</td>\n",
       "      <td>67.581948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikidata</th>\n",
       "      <td>757.0</td>\n",
       "      <td>0.997365</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.163373</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.163373</td>\n",
       "      <td>93.428571</td>\n",
       "      <td>59.155280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opencitations</th>\n",
       "      <td>757.0</td>\n",
       "      <td>0.997365</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.152833</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.117260</td>\n",
       "      <td>99.827586</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               success_count  success_rate  top_result_count  top_result_rate  \\\n",
       "openalex               757.0      0.997365             593.0         0.781291   \n",
       "matilda                757.0      0.997365             421.0         0.554677   \n",
       "wikidata               757.0      0.997365             161.0         0.212121   \n",
       "opencitations          757.0      0.997365             116.0         0.152833   \n",
       "\n",
       "               is_match_count  is_match_rate  year_match_count  \\\n",
       "openalex                383.0       0.504611             385.0   \n",
       "matilda                 314.0       0.413702             280.0   \n",
       "wikidata                124.0       0.163373             124.0   \n",
       "opencitations            88.0       0.115942              89.0   \n",
       "\n",
       "               year_match_rate  mean_title_similarity  mean_author_similarity  \n",
       "openalex              0.507246              87.416526               61.976391  \n",
       "matilda               0.368906              99.337292               67.581948  \n",
       "wikidata              0.163373              93.428571               59.155280  \n",
       "opencitations         0.117260              99.827586                0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_provider(records: list[dict], provider: str) -> Dict[str, Optional[float]]:\n",
    "    total_records = len(records)\n",
    "    success_count = 0\n",
    "    top_result_count = 0\n",
    "    match_count = 0\n",
    "    year_match_count = 0\n",
    "    title_sims: list[float] = []\n",
    "    author_sims: list[float] = []\n",
    "\n",
    "    for record in records:\n",
    "        metadata = (record.get(\"search_results\") or {}).get(provider, {}).get(\"metadata_search\")\n",
    "        if not metadata:\n",
    "            continue\n",
    "        if metadata.get(\"success\"):\n",
    "            success_count += 1\n",
    "        top_result = metadata.get(\"top_result\")\n",
    "        if not top_result:\n",
    "            continue\n",
    "        top_result_count += 1\n",
    "        if top_result.get(\"is_match\"):\n",
    "            match_count += 1\n",
    "        match_details = top_result.get(\"match_details\") or {}\n",
    "        title_sim = match_details.get(\"title_similarity\")\n",
    "        author_sim = match_details.get(\"author_similarity\")\n",
    "        if title_sim is not None:\n",
    "            title_sims.append(float(title_sim))\n",
    "        if author_sim is not None:\n",
    "            author_sims.append(float(author_sim))\n",
    "        if match_details.get(\"year_match\"):\n",
    "            year_match_count += 1\n",
    "\n",
    "    mean_title_sim = float(pd.Series(title_sims).mean()) if title_sims else None\n",
    "    mean_author_sim = float(pd.Series(author_sims).mean()) if author_sims else None\n",
    "\n",
    "    return {\n",
    "        \"success_count\": success_count,\n",
    "        \"success_rate\": success_count / total_records if total_records else 0.0,\n",
    "        \"top_result_count\": top_result_count,\n",
    "        \"top_result_rate\": top_result_count / total_records if total_records else 0.0,\n",
    "        \"is_match_count\": match_count,\n",
    "        \"is_match_rate\": match_count / total_records if total_records else 0.0,\n",
    "        \"year_match_count\": year_match_count,\n",
    "        \"year_match_rate\": year_match_count / total_records if total_records else 0.0,\n",
    "        \"mean_title_similarity\": mean_title_sim,\n",
    "        \"mean_author_similarity\": mean_author_sim,\n",
    "    }\n",
    "\n",
    "summary_main = {provider: summarize_provider(records, provider) for provider in providers_main}\n",
    "# Build stats for OpenCitations from its separate records\n",
    "summary_oc = {\"opencitations\": summarize_provider(records_opencitations, \"opencitations\")}\n",
    "summary = {**summary_main, **summary_oc}\n",
    "stats_df = pd.DataFrame(summary).T\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2a763",
   "metadata": {},
   "source": [
    "## Build Hugging Face dataset\n",
    "\n",
    "We expand every reference across the available providers and materialize a DatasetDict where each split key corresponds to the reference source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def347b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    openalex: Dataset({\n",
       "        features: ['ref_id', 'original_ref_string', 'matched_id', 'matched_doi', 'matched_result', 'is_match_by_similarity', 'matched_link'],\n",
       "        num_rows: 759\n",
       "    })\n",
       "    matilda: Dataset({\n",
       "        features: ['ref_id', 'original_ref_string', 'matched_id', 'matched_doi', 'matched_result', 'is_match_by_similarity', 'matched_link'],\n",
       "        num_rows: 759\n",
       "    })\n",
       "    wikidata: Dataset({\n",
       "        features: ['ref_id', 'original_ref_string', 'matched_id', 'matched_doi', 'matched_result', 'is_match_by_similarity', 'matched_link'],\n",
       "        num_rows: 759\n",
       "    })\n",
       "    opencitations: Dataset({\n",
       "        features: ['ref_id', 'original_ref_string', 'matched_id', 'matched_doi', 'matched_result', 'is_match_by_similarity', 'matched_link'],\n",
       "        num_rows: 759\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_openalex_id(raw_id: Optional[str]) -> Optional[str]:\n",
    "    if not raw_id:\n",
    "        return None\n",
    "    cleaned = raw_id.rstrip(\"/\")\n",
    "    return cleaned.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "def build_matched_id(provider: str, ids: Dict[str, Any]) -> Optional[str]:\n",
    "    if provider == \"openalex\":\n",
    "        return extract_openalex_id(ids.get(\"openalex_id\"))\n",
    "    if provider == \"matilda\":\n",
    "        return ids.get(\"matilda_id\")\n",
    "    if provider == \"wikidata\":\n",
    "        return ids.get(\"wikidata_id\")\n",
    "    if provider == \"opencitations\":\n",
    "        omid = ids.get(\"omid\")\n",
    "        if omid:\n",
    "            cleaned = omid.rstrip(\"/\")\n",
    "            parts = cleaned.split(\"/\")\n",
    "            # Return last two segments, e.g., \"br/06210459208\"\n",
    "            if len(parts) >= 2:\n",
    "                return \"/\".join(parts[-2:])\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_matched_link(provider: str, matched_id: Optional[str]) -> Optional[str]:\n",
    "    search_links = {\n",
    "        \"openalex\": \"https://openalex.org/\",\n",
    "        \"matilda\": \"https://matilda.science/?l=en\",\n",
    "        \"wikidata\": \"https://www.wikidata.org/wiki/Wikidata:Main_Page\",\n",
    "        \"opencitations\": \"https://sparql.opencitations.net/\",\n",
    "    }\n",
    "    if matched_id:\n",
    "        if provider == \"openalex\":\n",
    "            return f\"https://openalex.org/works?zoom={matched_id.lower()}\"\n",
    "        if provider == \"matilda\":\n",
    "            return f\"https://matilda.science/work/{matched_id}\"\n",
    "        if provider == \"wikidata\":\n",
    "            return f\"https://www.wikidata.org/wiki/{matched_id}\"\n",
    "        if provider == \"opencitations\":\n",
    "            return f\"https://api.opencitations.net/meta/v1/metadata/omid:{matched_id}\"\n",
    "    return search_links.get(provider)\n",
    "\n",
    "\n",
    "def resolve_is_match(metadata: Optional[Dict[str, Any]]) -> bool:\n",
    "    if not metadata:\n",
    "        return False\n",
    "    top_result = metadata.get(\"top_result\") or {}\n",
    "    is_match = top_result.get(\"is_match\")\n",
    "    if is_match is not None:\n",
    "        return bool(is_match)\n",
    "    match_details = top_result.get(\"match_details\") or {}\n",
    "    title_similarity = match_details.get(\"title_similarity\")\n",
    "    if title_similarity is None:\n",
    "        return False\n",
    "    return float(title_similarity) >= 90.0\n",
    "\n",
    "\n",
    "def summarize_top_result(top_result: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    fields = [\"title\", \"first_author\", \"year\", \"journal\"]\n",
    "    data = top_result or {}\n",
    "    return {field: data.get(field) for field in fields}\n",
    "\n",
    "\n",
    "\n",
    "def matched_result_to_text(m: Optional[Dict[str, Any]]) -> str:\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    parts = []\n",
    "    for k in (\"title\", \"first_author\", \"year\", \"journal\"):\n",
    "        v = m.get(k)\n",
    "        if v:\n",
    "            parts.append(f\"{k}: {v}\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "split_rows: dict[str, list[dict[str, Any]]] = defaultdict(list)\n",
    "\n",
    "# Build splits for the main providers from the combined records file\n",
    "for record in records:\n",
    "    base_row = {\n",
    "        \"ref_id\": record.get(\"ref_id\"),\n",
    "        \"original_ref_string\": record.get(\"original_string\"),\n",
    "        # \"reference_source\": record.get(\"source\"),\n",
    "    }\n",
    "    search_results = record.get(\"search_results\") or {}\n",
    "    for provider in providers_main:\n",
    "        metadata = (search_results.get(provider) or {}).get(\"metadata_search\")\n",
    "        top_result = metadata.get(\"top_result\") if metadata else None\n",
    "        ids = (top_result or {}).get(\"ids\") or {}\n",
    "        matched_id = build_matched_id(provider, ids)\n",
    "        row = {\n",
    "            **base_row,\n",
    "            \"matched_id\": matched_id or \"Not Found\",\n",
    "            \"matched_doi\": ids.get(\"doi\") or \"\",\n",
    "            # Store as dict/JSON for consistent schema across splits\n",
    "            \"matched_result\": summarize_top_result(top_result),\n",
    "            \"is_match_by_similarity\": resolve_is_match(metadata),\n",
    "            \"matched_link\": build_matched_link(provider, matched_id),\n",
    "        }\n",
    "        split_rows[provider].append(row)\n",
    "\n",
    "# Build split for OpenCitations from its separate records file\n",
    "for record in records_opencitations:\n",
    "    base_row = {\n",
    "        \"ref_id\": record.get(\"ref_id\"),\n",
    "        \"original_ref_string\": record.get(\"original_string\"),\n",
    "    }\n",
    "    metadata = (record.get(\"search_results\") or {}).get(\"opencitations\", {}).get(\"metadata_search\")\n",
    "    top_result = metadata.get(\"top_result\") if metadata else None\n",
    "    ids = (top_result or {}).get(\"ids\") or {}\n",
    "    matched_id = build_matched_id(\"opencitations\", ids)\n",
    "    row = {\n",
    "        **base_row,\n",
    "        \"matched_id\": matched_id or \"Not Found\",\n",
    "        \"matched_doi\": ids.get(\"doi\") or \"\",\n",
    "        # Store as dict/JSON for consistent schema across splits\n",
    "        \"matched_result\": summarize_top_result(top_result),\n",
    "        \"is_match_by_similarity\": resolve_is_match(metadata),\n",
    "        \"matched_link\": build_matched_link(\"opencitations\", matched_id),\n",
    "    }\n",
    "    split_rows[\"opencitations\"].append(row)\n",
    "\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    split: Dataset.from_list(rows)\n",
    "    for split, rows in split_rows.items()\n",
    "})\n",
    "\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc3287b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    openalex: Dataset({\n",
      "        features: ['ref_id', 'original_ref_string', 'matched_id', 'matched_doi', 'matched_result', 'is_match_by_similarity', 'matched_link'],\n",
      "        num_rows: 759\n",
      "    })\n",
      "    matilda: Dataset({\n",
      "        features: ['ref_id', 'original_ref_string', 'matched_id', 'matched_doi', 'matched_result', 'is_match_by_similarity', 'matched_link'],\n",
      "        num_rows: 759\n",
      "    })\n",
      "    wikidata: Dataset({\n",
      "        features: ['ref_id', 'original_ref_string', 'matched_id', 'matched_doi', 'matched_result', 'is_match_by_similarity', 'matched_link'],\n",
      "        num_rows: 759\n",
      "    })\n",
      "    opencitations: Dataset({\n",
      "        features: ['ref_id', 'original_ref_string', 'matched_id', 'matched_doi', 'matched_result', 'is_match_by_similarity', 'matched_link'],\n",
      "        num_rows: 759\n",
      "    })\n",
      "})\n",
      "Preview split: openalex\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ref_id': ['cex_BIO-GEN-MOL_9_1',\n",
       "  'cex_BIO-GEN-MOL_9_0',\n",
       "  'cex_BIO-GEN-MOL_9_3',\n",
       "  'cex_BIO-GEN-MOL_9_2',\n",
       "  'cex_BIO-GEN-MOL_9_4'],\n",
       " 'original_ref_string': ['L R Leddy, R E Holmes. Chondrosarcoma of bone. Cancer Treat Res. 2014. Vol. 162',\n",
       "  'A Y Giuffrida, J E Burgueno, L G Koniaris, J C Gutierrez, R Duncan, S P Scully. Chondrosarcoma in the United States (1973 to 2003): an analysis of 2890 cases from the SEER database. J Bone Joint Surg Am. 2009. Vol. 91',\n",
       "  'E Lhuissier, C Bazille, J Aury-Landas, N Girard, J Pontin, M Boittin, K Boumediene, C Baugé. Identification of an easy to use 3D culture model to investigate invasion and anticancer drug response in chondrosarcomas. BMC Cancer. 2017. Vol. 17',\n",
       "  'N Girard, C Bazille, E Lhuissier, H Benateau, A Llombart-Bosch, K Boumediene, C Baugé. 3-Deazaneplanocin A (DZNep), an Inhibitor of the Histone Methyltransferase EZH2, Induces Apoptosis and Reduces Cell Migration in Chondrosarcoma Cells. PloS One. 2014. Vol. 9. pp. e98176',\n",
       "  'J Puppe, R Drost, X Liu, S A Joosse, B Evers, P Cornelissen-Steijger, P Nederlof, Q Yu, J Jonkers, M van Lohuizen, A M Pietersen. BRCA1-deficient mammary tumor cells are dependent on EZH2 expression and sensitive to Polycomb Repressive Complex 2-inhibitor 3-deazaneplanocin. A. Breast Cancer Res. 2009. Vol. 11. pp. R63'],\n",
       " 'matched_id': ['W322718260',\n",
       "  'W2098245740',\n",
       "  'W2737276581',\n",
       "  'W1983545974',\n",
       "  'W2133743991'],\n",
       " 'matched_doi': ['',\n",
       "  'https://doi.org/10.2106/jbjs.h.00416',\n",
       "  'https://doi.org/10.1186/s12885-017-3478-z',\n",
       "  'https://doi.org/10.1371/journal.pone.0098176',\n",
       "  'https://doi.org/10.1186/bcr2354'],\n",
       " 'matched_result': [{'first_author': 'Lichtenstein',\n",
       "   'journal': 'PubMed',\n",
       "   'title': 'Chondrosarcoma of Bone.',\n",
       "   'year': 1943},\n",
       "  {'first_author': 'Giuffrida',\n",
       "   'journal': 'Journal of Bone and Joint Surgery',\n",
       "   'title': 'Chondrosarcoma in the United States (1973 to 2003): An Analysis of 2890 Cases from the SEER Database',\n",
       "   'year': 2009},\n",
       "  {'first_author': 'Lhuissier',\n",
       "   'journal': 'BMC Cancer',\n",
       "   'title': 'Identification of an easy to use 3D culture model to investigate invasion and anticancer drug response in chondrosarcomas',\n",
       "   'year': 2017},\n",
       "  {'first_author': 'Girard',\n",
       "   'journal': 'PLoS ONE',\n",
       "   'title': '3-Deazaneplanocin A (DZNep), an Inhibitor of the Histone Methyltransferase EZH2, Induces Apoptosis and Reduces Cell Migration in Chondrosarcoma Cells',\n",
       "   'year': 2014},\n",
       "  {'first_author': 'Puppe',\n",
       "   'journal': 'Breast Cancer Research',\n",
       "   'title': 'BRCA1-deficient mammary tumor cells are dependent on EZH2 expression and sensitive to Polycomb Repressive Complex 2-inhibitor 3-deazaneplanocin A',\n",
       "   'year': 2009}],\n",
       " 'is_match_by_similarity': [False, True, True, True, True],\n",
       " 'matched_link': ['https://openalex.org/works?zoom=w322718260',\n",
       "  'https://openalex.org/works?zoom=w2098245740',\n",
       "  'https://openalex.org/works?zoom=w2737276581',\n",
       "  'https://openalex.org/works?zoom=w1983545974',\n",
       "  'https://openalex.org/works?zoom=w2133743991']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset_dict)\n",
    "preview_split = next(iter(dataset_dict.keys()))\n",
    "print(f\"Preview split: {preview_split}\")\n",
    "dataset_dict[preview_split][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de49342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792db7ab911d4ca4bac2b9f48c792edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0382d7cef34b7994bb488a08940fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ba5e4acda54d28b400ab31b65934d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168bd3b8a4744bdcac67a9d192e55643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3949a8427314942aee8520a2a4419c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        : 100%|##########|  148kB /  148kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6245c1f38333423d813b3777a27ac405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de21464cc2a4968bfea03e70cb4b06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b296ce8d04b84e33bcf6c25ef575ff5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5d1ce47060413482cd64c597744d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9fbe0d7b454575bf5d65a895c178f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        : 100%|##########|  243kB /  243kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b933fbd9bf544cacae8867b2a2640ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c495d9f3fc048b0935256d06d5802ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ae85eff68d4dc58a487da92ff7e1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b966b540f5e24af3860557569213a84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e70387603d45a58bafa1edde57ed56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        : 100%|##########| 98.5kB / 98.5kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ef2bd4ead847db965fc62c2286eac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbe801e765f4ed6833459f609190209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b7ac0aa5854c6fbe6b6acc5d42b0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f34ace0cff42bd90b891b4a28f7047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcc01b8d9b24794ac51f035385bd6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  33%|###3      | 52.7kB /  159kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502efe464612453d86cb0734f6f72c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/881 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/yurui983/citation_linking/commit/198d81017ef579553969ff004655bcbd87302b14', commit_message='Upload dataset', commit_description='', oid='198d81017ef579553969ff004655bcbd87302b14', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/yurui983/citation_linking', endpoint='https://huggingface.co', repo_type='dataset', repo_id='yurui983/citation_linking'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "# matched_result is now stored as dict/JSON (not string) for better type consistency\n",
    "dataset_dict.push_to_hub(\"yurui983/citation_linking\", private=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1818e",
   "metadata": {},
   "source": [
    "## Push to Argilla for annotation\n",
    "\n",
    "This section creates one Argilla dataset per provider split and uploads records for human annotation. Each record will include:\n",
    "\n",
    "- the original reference string\n",
    "- the matched id and a link to the matched result\n",
    "- a compact matched result summary (title, first_author, year, journal)\n",
    "\n",
    "The annotation questions are:\n",
    "1) Is the matched result correct? (true/false)\n",
    "2) If incorrect, provide the correct ID (text input).\n",
    "\n",
    "You will be prompted for Argilla API URL and API token to connect to your deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67312d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install argilla if not already installed (uncomment to run in notebook)\n",
    "# !pip install -U argilla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5f57b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidelines_markdown = \"\"\"\n",
    "# Annotation Guidelines: Citation Linking\n",
    "\n",
    "## What the fields mean\n",
    "- ref_id: Internal identifier of the reference.\n",
    "- original_ref_string: The raw reference text extracted from the document.\n",
    "- matched_id: Provider-specific ID of the candidate match (or \"Not Found\").\n",
    "  - OpenAlex: W1234567890 (last segment of /works/W…)\n",
    "  - Wikidata: Q12345\n",
    "  - Matilda: last path segment in /work/<id>\n",
    "  - OpenCitations: OMID last path segment (e.g., br/06210459208)\n",
    "- matched_doi: DOI of the candidate match (if available).\n",
    "- matched_result: Compact summary of the candidate (title, first_author, year, journal).\n",
    "- is_match_by_similarity: Model’s heuristic guess (for context only).\n",
    "- matched_link: Link to view the candidate on the provider site.\n",
    "\n",
    "## What you need to do\n",
    "1) Check if the candidate is the same work as the reference.\n",
    "   - Compare title, first author surname, year, and DOI (if present).\n",
    "   - Use matched_link to verify details on the provider page.\n",
    "2) If the candidate is incorrect OR empty (matched_id = \"Not Found\" or no details shown), find the correct record in the same provider and paste its ID in correct_id.\n",
    "3) If you believe no record exists for this reference in this provider, select “No match”.\n",
    "\n",
    "## How to answer\n",
    "- Candidate is correct:\n",
    "  - is_match_correct = true\n",
    "  - No match = false\n",
    "  - correct_id = (leave blank)\n",
    "- Candidate is incorrect but correct record exists:\n",
    "  - is_match_correct = false\n",
    "  - No match = false\n",
    "  - correct_id = provider-specific ID (OpenAlex W…, Wikidata Q…, Matilda work id, OpenCitations OMID br/…)\n",
    "- No record in this provider:\n",
    "  - is_match_correct = false\n",
    "  - No match = true\n",
    "  - correct_id = (leave blank)\n",
    "\n",
    "Notes:\n",
    "- Minor formatting/casing differences are fine; it must be the same work.\n",
    "- Provide only the ID (not a URL).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74fed502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset: citation_linking_opencitations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/citation_index/lib/python3.11/site-packages/argilla/datasets/_resource.py:264: UserWarning: Workspace not provided. Using default workspace: default id: d4a1881b-5e03-4d0b-b42e-d18e4f94b1af\n",
      "  warnings.warn(f\"Workspace not provided. Using default workspace: {workspace.name} id: {workspace.id}\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/citation_index/lib/python3.11/site-packages/argilla/records/_mapping/_mapper.py:89: UserWarning: Keys ['metadata'] in data are not present in the mapping and will be ignored.\n",
      "  warnings.warn(f\"Keys {unknown_keys} in data are not present in the mapping and will be ignored.\")\n",
      "Sending records...: 3batch [00:06,  2.30s/batch]                    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    import argilla as rg\n",
    "except Exception:\n",
    "    raise\n",
    "\n",
    "ARGILLA_API_URL = os.environ.get(\"ARGILLA_API_URL\") or 'https://argilla-route-graphia-app1-staging.apps.bst2.paas.psnc.pl/'\n",
    "ARGILLA_API_TOKEN = os.environ.get(\"ARGILLA_API_TOKEN\") or 'argilla.apikey'\n",
    "\n",
    "client = rg.Argilla(api_key=ARGILLA_API_TOKEN, api_url=ARGILLA_API_URL)\n",
    "# 'ref_id', 'original_ref_string', 'matched_id', 'matched_doi', 'matched_result', 'is_match_by_similarity', 'matched_link'],\n",
    "\n",
    "settings = rg.Settings(\n",
    "        fields=[\n",
    "            rg.TextField(name=\"ref_id\", title=\"Reference ID\", description=\"The ID of the original reference.\"),\n",
    "            rg.TextField(name=\"original_ref_string\", title=\"Reference String\", description=\"The original reference string extracted from the document.\"),\n",
    "            rg.TextField(name=\"matched_id\", title=\"Matched ID\", description=\"The ID of the matched record from the external provider.\"),\n",
    "            rg.TextField(name=\"matched_doi\", title=\"Matched DOI\", description=\"The DOI of the matched record from the external provider.\"),\n",
    "            rg.TextField(name=\"matched_result\", title=\"Matched Result\", description=\"The result of the matching process.\",use_markdown=True),\n",
    "            rg.TextField(name=\"is_match_by_similarity\", title=\"Is Match by Similarity\", description=\"Indicates if the match was found by similarity.\"),\n",
    "            rg.TextField(name=\"matched_link\", title=\"Matched Link\", use_markdown=True, description=\"The link to the matched record from the external provider.\"),\n",
    "        ],\n",
    "        guidelines=guidelines_markdown,\n",
    "        questions=[\n",
    "            rg.LabelQuestion(\n",
    "                name=\"is_match_correct\",\n",
    "                title=\"Is the matched result correct?\",\n",
    "                labels=[\"true\", \"false\"],\n",
    "                description=\"Select 'true' if the candidate matched record corresponds to the same publication as the reference (matching title, first author and year). Otherwise select 'false'.\",\n",
    "                required=True,\n",
    "            ),  \n",
    "            rg.TextQuestion(\n",
    "                name=\"correct_id\",\n",
    "                title=\"If incorrect, provide the correct ID (only id)\",\n",
    "                description=\"When the candidate is incorrect, type the provider-specific identifier of the correct match (e.g., OpenAlex id like W1234567890). Leave blank if 'true'.\",\n",
    "                required=False,\n",
    "            ),\n",
    "            rg.LabelQuestion(\n",
    "                name=\"This reference has no match\",\n",
    "                title=\"Does this reference have no match?\",\n",
    "                labels=[\"true\", \"false\"],\n",
    "                description=\"Select 'true' if the reference has no match in the this provider. Otherwise select 'false'.\",\n",
    "                required=True,\n",
    "            ),\n",
    "            ]\n",
    "            \n",
    ")\n",
    "\n",
    "# Helper to format matched_result into a readable string\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "\n",
    "def matched_result_to_markdown_json(m: Optional[Dict[str, Any]]) -> str:\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    \n",
    "    json_str = '```json\\n' + json.dumps(m, indent=2) + '\\n```'\n",
    "    return json_str\n",
    "\n",
    "# Define the Argilla dataset schema (fields/questions)\n",
    "# from argilla.client.models import TextField, TextAreaField, LabelQuestion, ResponseSchema, FieldSchema\n",
    "\n",
    "def matched_link_to_markdown(m: Optional[str]) -> str:\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    return f\"[{m}]({m})\"\n",
    "\n",
    "# Use the local dataset_dict (already created in Cell 5)\n",
    "# If you want to load from hub instead, use: load_dataset(\"yurui983/citation_linking\")\n",
    "dataset_for_argilla = dataset_dict\n",
    "\n",
    "for split_name, ds in dataset_for_argilla.items():\n",
    "\n",
    "    if split_name != 'opencitations':\n",
    "        continue\n",
    "\n",
    "    dataset_name = f\"citation_linking_{split_name}\"\n",
    "\n",
    "    #delete existing dataset if exists\n",
    "    # dataset = rg.Dataset(name=dataset_name)\n",
    "    # dataset.delete()\n",
    "    print(\"Creating dataset:\", dataset_name)\n",
    "    \n",
    "    dataset = rg.Dataset(name=dataset_name, settings=settings)\n",
    "    dataset.create()\n",
    "    hf_dataset = dataset_for_argilla[split_name]\n",
    "    records_to_log = []\n",
    "    for ex in hf_dataset:\n",
    "        matched_raw = ex.get(\"matched_result\") or {}\n",
    "        # If matched_result is a dict with the summary fields, convert to text\n",
    "        matched_text = (\n",
    "            matched_result_to_markdown_json(matched_raw)\n",
    "            if isinstance(matched_raw, dict)\n",
    "            else str(matched_raw or \"\")\n",
    "        )\n",
    "\n",
    "        is_match_val = ex.get(\"is_match_by_similarity\")\n",
    "        # Argilla TextField expects string (or list/dict depending on schema); convert bool to string\n",
    "        if isinstance(is_match_val, bool):\n",
    "            is_match_str = \"true\" if is_match_val else \"false\"\n",
    "        # else:\n",
    "        #     is_match_str = str(is_match_val) if is_match_val is not None else \"\"\n",
    "\n",
    "        record = {\n",
    "            \"ref_id\": ex.get(\"ref_id\") or \"\",\n",
    "            \"original_ref_string\": ex.get(\"original_ref_string\") or \"\",\n",
    "            \"matched_id\": ex.get(\"matched_id\") or \"Not Found\",\n",
    "            \"matched_doi\": ex.get(\"matched_doi\") or \"\",\n",
    "            \"matched_result\": matched_text,\n",
    "            \"is_match_by_similarity\": is_match_str,\n",
    "            \"matched_link\": ex.get(\"matched_link\") or \"\",\n",
    "            # include ref_id as metadata so annotators / export scripts can trace back\n",
    "            \"metadata\": {\"ref_id\": ex.get(\"ref_id\")},\n",
    "        }\n",
    "        records_to_log.append(record)\n",
    "\n",
    "    # Log records (batch)\n",
    "    # ds.records.log(records=records_to_log)\n",
    "    dataset.records.log(records=records_to_log)\n",
    "\n",
    "    # break  # Only do one split for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba45bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citation_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
